{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb60025-9f39-4942-b890-468cba64e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:20:49.479165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-18 02:20:52.291676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import AutoAugmentPolicy, AutoAugment\n",
    "from torchvision.transforms.v2 import MixUp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0804b4-4662-443f-b37e-abd95dbf438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f90785-de5a-4607-b3de-b0221fa098cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c4fcab-4cf8-4b49-99f9-fe7b1e43f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Function to evaluate the neural network on the test data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += torch.sum(predicted_labels == labels).item()\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_labels / total_labels\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75d5800-aeef-4b53-bcb8-2bb11aa0aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Function to train the neural network for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "        total_labels += labels.size(0)\n",
    "        correct_labels += torch.sum(predicted_labels == labels).item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_labels / total_labels\n",
    "    \n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "315902d4-964f-4054-bd8f-31067163603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_resnet50(lr, num_epochs, train_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Fine-tune ResNet-50 for binary classification, logging training details to TensorBoard.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the pre-trained ResNet-50 model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Modify the final fully connected layer for binary classification\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 2)  # 2 output classes\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()             # Loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # Optimizer\n",
    "    \n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss, train_accuracy = train_nn(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_accuracy = evaluate_nn(model, test_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Write to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", train_accuracy, epoch)\n",
    "        writer.add_scalar(\"Accuracy/test\", test_accuracy, epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    # Returns the model and metrics\n",
    "    return model, train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd1785-c75c-486d-adc9-0b246d77b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_resnet50(lr, num_epochs, train_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Fine-tune ResNet-50 for binary classification with Cosine Annealing LR scheduling, logging details to TensorBoard.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the pre-trained ResNet-50 model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Modify the final fully connected layer for binary classification\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 2)  # 2 output classes\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()                   # Loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)   # Optimizer\n",
    "    \n",
    "    # Initialize the cosine annealing scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss, train_accuracy = train_nn(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_accuracy = evaluate_nn(model, test_loader, criterion, device)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log training metrics\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", train_accuracy, epoch)\n",
    "        writer.add_scalar(\"Accuracy/test\", test_accuracy, epoch)\n",
    "\n",
    "        # Log the current learning rate to TensorBoard\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        writer.add_scalar(\"Learning Rate\", current_lr, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    # Returns the model and metrics\n",
    "    return model, train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff52cf80-b9a7-4d1b-8f4d-094cbe0c4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_resnet50(lr, num_epochs, train_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Fine-tune ResNet-50 for binary classification with Cosine Annealing LR scheduling, MixUp, and logging details to TensorBoard.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the pre-trained ResNet-50 model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Modify the final fully connected layer for binary classification\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 2)  # 2 output classes\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()                   # Loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)   # Optimizer\n",
    "    \n",
    "    # Initialize Cosine Annealing Scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    \n",
    "    # Initialize MixUp transform using torchvision or an equivalent library\n",
    "    mixup_transform = MixUp(num_classes=2, alpha=1.0)\n",
    "    \n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Apply MixUp transformation if available in transforms\n",
    "            inputs, labels = mixup_transform(inputs, labels)\n",
    "            # print(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate mixup loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            # total_labels += labels.size(0)\n",
    "            # correct_labels += (lam * (predicted_labels == labels_a).sum().item() +\n",
    "            #                    (1 - lam) * (predicted_labels == labels_b).sum().item())\n",
    "\n",
    "        # Calculate average train loss and accuracy\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        # train_accuracy = correct_labels / total_labels\n",
    "\n",
    "        # Evaluate on test data\n",
    "        test_loss, test_accuracy = evaluate_nn(model, test_loader, criterion, device)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log metrics\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        # writer.add_scalar(\"Accuracy/train\", train_accuracy, epoch)\n",
    "        writer.add_scalar(\"Accuracy/test\", test_accuracy, epoch)\n",
    "\n",
    "        # Log the current learning rate to TensorBoard\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        writer.add_scalar(\"Learning Rate\", current_lr, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        # print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    # Returns the model and metrics\n",
    "    return model, train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd88b96-0a06-493e-855d-6a159cb62be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/ahmad/courses/cuda_lab/MA-INF-4308-Lab-Vision-Systems/Assignment-3/dataset'\n",
    "log_dir = './runs/human_robot_classifier_' + datetime.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6d5f8a-c657-4d7e-b64f-2b56bff7b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# Set random seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19682316-250c-4152-80cd-ebf78a272f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "    ], p=0.8),\n",
    "    # AutoAugment(AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7cd89ab-a6f9-4e73-bca6-394ba9e9cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19495554-510c-49cd-b93a-ad9e05316b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['person', 'robot'], ['person', 'robot'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes, val_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792adc12-7a5d-45bc-a509-f1bd03a34e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d065e75-4890-404e-bb60-3e5de02458d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mixup transform\n",
    "mixup_transform = MixUp(num_classes=2, alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac6c7b-9351-4722-905d-cb063cf02776",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8091394-8c61-477f-8a5b-136b45cdc86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: human and robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e8fd05e-ca12-48cf-8aca-9e59f572316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f125c74-44be-476f-81d7-8ad312e27591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensorboard writer\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90004565-24f6-4e33-bbff-cbd8c87c4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cbe0ca5-02e4-426d-8c50-c01fae44fee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                            | 0/10 [01:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://www.kaggle.com/code/ar2017/pytorch-efficientnet-train-aug-cutmix-fmix --> see this for cutmix\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_training_resnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 73\u001b[0m, in \u001b[0;36mrun_training_resnet50\u001b[0;34m(lr, num_epochs, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     70\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     71\u001b[0m test_losses\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[0;32m---> 73\u001b[0m train_accuracies\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_accuracy\u001b[49m)\n\u001b[1;32m     74\u001b[0m test_accuracies\u001b[38;5;241m.\u001b[39mappend(test_accuracy)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Log metrics to TensorBoard\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/ar2017/pytorch-efficientnet-train-aug-cutmix-fmix --> see this for cutmix\n",
    "run_training_resnet50(lr, num_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "272f6db7-8bfc-41c9-89fa-a04ea5f37390",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixup_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[21], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, mixup_transform, num_epochs, writer)\u001b[0m\n\u001b[1;32m     34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     35\u001b[0m     _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m)\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 20\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, mixup_transform, num_epochs, writer)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06018924-65a6-4434-a730-56e39656987a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
