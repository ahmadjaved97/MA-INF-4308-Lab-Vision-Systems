{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc431750-51c9-4788-8bc5-cb7349f81892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 18:21:17.788976: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-18 18:21:18.761102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33563e7e-30f5-4910-a8fc-7be18597bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a590f238-2c23-43cb-a57a-c1820a8290fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CutMix function\n",
    "# Used from the link given in the paper: https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "def cutmix(inputs, labels, alpha=1.0):\n",
    "    \"\"\"Applies CutMix augmentation.\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = inputs.size(0)\n",
    "    rand_index = torch.randperm(batch_size).to(inputs.device)\n",
    "    shuffled_labels = labels[rand_index]\n",
    "    \n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "    inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "    \n",
    "    return inputs, labels, shuffled_labels, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5079baf9-725b-439d-8940-60a25cfa55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    \"\"\"Generate a random bounding box for CutMix.\"\"\"\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1.0 - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c355af-eec1-4644-97c9-998f76396310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train_nn(model, train_loader, criterion, optimizer, device, cutmix_prob=0.5, cutmix_alpha=1.0):\n",
    "    \"\"\"Function to train the neural network for one epoch with optional CutMix.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if np.random.rand() < cutmix_prob:\n",
    "            inputs, labels_a, labels_b, lam = cutmix(inputs, labels, alpha=cutmix_alpha)\n",
    "            outputs = model(inputs)\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "        total_labels += labels.size(0)\n",
    "        correct_labels += torch.sum(predicted_labels == labels).item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_labels / total_labels\n",
    "    \n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8fec0c9-e9e2-43db-8900-2806ffe825cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_nn(model, test_loader, criterion, device):\n",
    "    \"\"\"Function to evaluate the neural network on the test data.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += torch.sum(predicted_labels == labels).item()\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_labels / total_labels\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9cd4beb-0350-45b5-97d1-0daad9615b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_name, num_classes):\n",
    "    \"\"\"\n",
    "    Prepare a model for training, either as a feature extractor or for fine-tuning.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained model\n",
    "    if model_name == \"resnet\":\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    elif model_name == \"squeezenet\":\n",
    "        model = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.DEFAULT)\n",
    "    elif model_name == \"convnext\":\n",
    "        model = models.convnext_base(weights=models.ConvNeXt_Base_Weights.IMAGENET1K_V1)\n",
    "    elif model_name == \"vit\":\n",
    "        model = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "    elif model_name == \"mobilenet\":\n",
    "        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "    \n",
    "    # Modify the last layer for the dataset\n",
    "    if model_name == \"squeezenet\":\n",
    "        model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "    elif model_name == \"convnext\":\n",
    "        model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n",
    "    elif model_name == \"vit\":\n",
    "        model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
    "    elif model_name == \"mobilenet\":\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    else:  # ResNet\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # # If using as a fixed feature extractor, freeze all layers except the final layer\n",
    "    # if feature_extractor:\n",
    "    #     for param in model.parameters():\n",
    "    #         param.requires_grad = False\n",
    "        \n",
    "    #     # Enable gradients only for the last layer\n",
    "    #     if model_name == \"squeezenet\":\n",
    "    #         for param in model.classifier[1].parameters():\n",
    "    #             param.requires_grad = True\n",
    "    #     elif model_name == \"convnext\":\n",
    "    #         for param in model.classifier[2].parameters():\n",
    "    #             param.requires_grad = True\n",
    "    #     elif model_name == \"vit\":\n",
    "    #         for param in model.heads.head.parameters():\n",
    "    #             param.requires_grad = True\n",
    "    #     else:  # ResNet\n",
    "    #         for param in model.fc.parameters():\n",
    "    #             param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d891c78f-5f2a-4e13-840d-8ee7021a346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    model_name,\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    lr,\n",
    "    num_epochs,\n",
    "    num_classes,\n",
    "    cutmix_alpha=1.0,\n",
    "    cutmix_prob=0.5,\n",
    "    weight_decay=1e-4,\n",
    "    log_dir=\"runs\",\n",
    "    save_path=\"saved_models\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run training for a model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model.\n",
    "        model: An instance of the model.\n",
    "        train_loader: DataLoader for training data.\n",
    "        test_loader: DataLoader for testing data.\n",
    "        lr (float): Learning rate.\n",
    "        num_epochs (int): Number of epochs.\n",
    "        num_classes (int): Number of classes in the dataset.\n",
    "        alpha (float): Mixup parameter.\n",
    "        cutmix_prob (float): Probability of applying CutMix.\n",
    "        weight_decay (float): Weight decay for optimizer.\n",
    "        log_dir (str): Directory for TensorBoard logs.\n",
    "        save_path (str): Directory to save the trained model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer and learning rate scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    train_losses, train_accuracies = [], []\n",
    "    test_losses, test_accuracies = [], []\n",
    "    learning_rates = []\n",
    "\n",
    "    # TensorBoard setup\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Training\n",
    "        train_loss, train_accuracy = train_nn(model, train_loader, criterion, optimizer, device, cutmix_alpha, cutmix_prob)\n",
    "        # Evaluation\n",
    "        test_loss, test_accuracy = evaluate_nn(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Test\", test_accuracy, epoch)\n",
    "        writer.add_scalar(\"Learning_Rate\", current_lr, epoch)\n",
    "\n",
    "        # Logging to console\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Save the model\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model_save_path = os.path.join(save_path, f\"{model_name}_final.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    return model, train_losses, train_accuracies, test_losses, test_accuracies, learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3729044b-0c3b-40bb-8662-fb39a00f38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_training(\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    lr,\n",
    "    fixed_num_epochs,\n",
    "    full_num_epochs,\n",
    "    num_classes,\n",
    "    freeze_feature_extractor=False,\n",
    "    full_finetune=False,\n",
    "    weight_decay=1e-4,\n",
    "    cutmix_alpha=1.0,\n",
    "    cutmix_prob=0.5,\n",
    "    log_dir=\"runs\",\n",
    "    save_path=\"saved_models\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Combined training function for using the model as a fixed feature extractor\n",
    "    and/or fully fine-tuning the model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model.\n",
    "        train_loader: DataLoader for training data.\n",
    "        test_loader: DataLoader for testing data.\n",
    "        lr (float): Learning rate.\n",
    "        num_epochs (int): Number of epochs.\n",
    "        num_classes (int): Number of classes in the dataset.(now redundant)\n",
    "        freeze_feature_extractor (bool): Whether to freeze feature extractor layers.\n",
    "        full_finetune (bool): Whether to fine-tune the entire model.\n",
    "        weight_decay (float): Weight decay for optimizer.\n",
    "        cutmix_alpha (float): Mixup parameter.\n",
    "        cutmix_prob (float): Probability of applying CutMix.\n",
    "        log_dir (str): Directory for TensorBoard logs.\n",
    "        save_path (str): Directory to save the trained model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Number of classes\n",
    "    num_classes = len(train_loader.dataset.classes)\n",
    "\n",
    "    # Prepare the model\n",
    "    model = prepare_model(model_name, num_classes)\n",
    "\n",
    "    combined_train_losses = []\n",
    "    combined_train_accuracies = []\n",
    "    combined_test_losses = []\n",
    "    combined_test_accuracies = []\n",
    "\n",
    "    # If freeze_feature_extractor is enabled\n",
    "    if freeze_feature_extractor:\n",
    "        print(f\"Using {model_name} as a fixed feature extractor.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "        # Check if all parameters are frozen\n",
    "        all_frozen = all(not param.requires_grad for param in model.parameters())\n",
    "        print(f\"Are all weights frozen? {all_frozen}\")\n",
    "\n",
    "        # # Unfreeze the classifier layer\n",
    "        # for param in model.fc.parameters():\n",
    "        #     param.requires_grad = True\n",
    "\n",
    "        # num_logits = model.fc.in_features\n",
    "        # model.fc = nn.Linear(num_logits, num_classes)\n",
    "\n",
    "        # Update the classification head based on model architecture\n",
    "        if hasattr(model, \"fc\"):  # Models with fully connected layers (e.g., ResNet)\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        elif hasattr(model, \"classifier\"):  # Models like AlexNet, ConvNeXt, MobileNet, SqueezeNet\n",
    "            if model_name == \"mobilenet\":  # MobileNet specific adjustment\n",
    "                model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "            elif model_name == \"squeezenet\":  # SqueezeNet specific adjustment\n",
    "                model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "            else:  # Generic case for other classifier-based models\n",
    "                model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)\n",
    "        elif hasattr(model, \"heads\"):  # Vision Transformer (ViT)\n",
    "            model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model architecture for {model_name}\")\n",
    "\n",
    "\n",
    "        print(\"Training with fixed feature extractor...\")\n",
    "        start_time = time.time()\n",
    "        model, train_losses, train_accuracies, test_losses, test_accuracies, learning_rates = run_training(\n",
    "            model_name=model_name,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            lr=lr,\n",
    "            num_epochs=fixed_num_epochs,\n",
    "            num_classes=num_classes,\n",
    "            cutmix_alpha=cutmix_alpha,\n",
    "            cutmix_prob=cutmix_prob,\n",
    "            weight_decay=weight_decay,\n",
    "            log_dir=f\"{log_dir}/fixed_feature_extractor\",\n",
    "            save_path=f\"{save_path}/fixed_feature_extractor\",\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        # writer.add_scalar(\"Training Time/Fixed Feature Extractor\", elapsed_time)\n",
    "        print(f\"Fixed feature extractor training completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        combined_train_losses.extend(train_losses)\n",
    "        combined_test_losses.extend(test_losses)\n",
    "        combined_train_accuracies.extend(train_accuracies)\n",
    "        combined_test_accuracies.extend(test_accuracies)\n",
    "\n",
    "    # If full_finetune is enabled\n",
    "    if full_finetune:\n",
    "        print(f\"Fine-tuning the entire {model_name} model.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Verify if all parameters are set to requires_grad = True\n",
    "        all_trainable = all(param.requires_grad for param in model.parameters())\n",
    "        print(f\"All parameters set to requires_grad=True: {all_trainable}\")\n",
    "\n",
    "        print(\"Training with full fine-tuning...\")\n",
    "        start_time = time.time()\n",
    "        model, train_losses, train_accuracies, test_losses, test_accuracies, learning_rates = run_training(\n",
    "            model_name=model_name,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            lr=lr * 0.5,                            # Lowering the learning rate\n",
    "            num_epochs=full_num_epochs,\n",
    "            num_classes=num_classes,\n",
    "            cutmix_alpha=cutmix_alpha,\n",
    "            cutmix_prob=cutmix_prob,\n",
    "            weight_decay=weight_decay,\n",
    "            log_dir=f\"{log_dir}/full_finetune\",\n",
    "            save_path=f\"{save_path}/full_finetune\",\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        # writer.add_scalar(\"Training Time/Full Fine-Tuning\", elapsed_time)\n",
    "        print(f\"Full fine-tuning completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "        combined_train_losses.extend(train_losses)\n",
    "        combined_test_losses.extend(test_losses)\n",
    "        combined_train_accuracies.extend(train_accuracies)\n",
    "        combined_test_accuracies.extend(test_accuracies)\n",
    "    \n",
    "    return model, combined_train_losses, combined_test_losses, combined_train_accuracies, combined_test_accuracies\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    # writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3c956ee-5cfc-4a08-bf6c-8f1a20221125",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/ahmad/courses/cuda_lab/MA-INF-4308-Lab-Vision-Systems/Assignment-3/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f3df682-99a1-498d-bf88-bee004df9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# Set random seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85b179b1-cb2b-4752-9253-e8a32d593625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "    ], p=0.8),\n",
    "    # AutoAugment(AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c6bcf47-e3bd-4ef8-addd-973d5a6e5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f30e05d4-f037-4f14-9057-b76bffbf3d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['person', 'robot'], ['person', 'robot'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes, val_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3eda0564-a32b-4178-a0cc-77205b162751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e967ed-ecb5-47b3-b85f-cfb8750d7fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using convnext as a fixed feature extractor.\n",
      "Are all weights frozen? True\n",
      "Training with fixed feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████▊                                                                                                                       | 1/5 [01:19<05:17, 79.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5121, Train Accuracy: 0.8294\n",
      "Test Loss: 0.2149, Test Accuracy: 0.9833\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████████████████▌                                                                                         | 2/5 [02:53<04:24, 88.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.4086, Train Accuracy: 0.8118\n",
      "Test Loss: 0.1105, Test Accuracy: 1.0000\n",
      "Learning Rate: 0.000905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████▍                                                           | 3/5 [04:39<03:12, 96.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.3641, Train Accuracy: 0.8794\n",
      "Test Loss: 0.0834, Test Accuracy: 1.0000\n",
      "Learning Rate: 0.000655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                             | 4/5 [06:24<01:39, 99.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.3997, Train Accuracy: 0.8441\n",
      "Test Loss: 0.0782, Test Accuracy: 1.0000\n",
      "Learning Rate: 0.000345\n"
     ]
    }
   ],
   "source": [
    "model, combined_train_losses, combined_test_losses, combined_train_accuracies, combined_test_accuracies = combine_training(\n",
    "    model_name=\"convnext\",\n",
    "    train_loader=train_loader,\n",
    "    test_loader=val_loader,\n",
    "    lr=0.001,\n",
    "    fixed_num_epochs=5,\n",
    "    full_num_epochs=20,\n",
    "    num_classes=2,\n",
    "    freeze_feature_extractor=True,\n",
    "    full_finetune=True,\n",
    "    log_dir=\"logs\",\n",
    "    save_path=\"models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c3da1-eca8-4332-93a8-fb80c376a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(combined_train_losses, label='Train Loss', marker='o', markeredgecolor='black')\n",
    "plt.plot(combined_test_losses, label='Test Loss', marker='o', markeredgecolor='black')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Model metrics - Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(combined_train_accuracies, label='Train Accuracy', marker='o', markeredgecolor='black')\n",
    "plt.plot(combined_test_accuracies, label='Test Accuracy', marker='o', markeredgecolor='black')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model metrics - Accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.plot(learning_rates, label='Learning Rate', marker='o', markeredgecolor='black', color='blue')\n",
    "# # plt.plot(test_accuracies, label='Test Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Learning Rate')\n",
    "# plt.legend()\n",
    "# plt.title('Model metrics - LR')\n",
    "# plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a5f4e-1da3-42d5-8da0-fe26a369bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions and true labels\n",
    "def get_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_predictions), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3c250-71dc-4a04-affe-8a29ad1c7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "y_pred, y_true = get_predictions(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276cd65-0a00-4de1-87c6-2c928a503fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['person', 'robot']\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_names, yticklabels=label_names, linewidths=1)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Label Names')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b6b10-3eaa-4eb9-a65c-48b21e8b8507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
