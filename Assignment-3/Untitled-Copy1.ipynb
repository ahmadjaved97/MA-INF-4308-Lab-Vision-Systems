{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc431750-51c9-4788-8bc5-cb7349f81892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33563e7e-30f5-4910-a8fc-7be18597bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590f238-2c23-43cb-a57a-c1820a8290fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CutMix function\n",
    "# Used from the link given in the paper: https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "def cutmix(inputs, labels, alpha=1.0):\n",
    "    \"\"\"Applies CutMix augmentation.\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = inputs.size(0)\n",
    "    rand_index = torch.randperm(batch_size).to(inputs.device)\n",
    "    shuffled_labels = labels[rand_index]\n",
    "    \n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "    inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "    \n",
    "    return inputs, labels, shuffled_labels, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5079baf9-725b-439d-8940-60a25cfa55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    \"\"\"Generate a random bounding box for CutMix.\"\"\"\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1.0 - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c355af-eec1-4644-97c9-998f76396310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train_nn(model, train_loader, criterion, optimizer, device, cutmix_prob=0.5, cutmix_alpha=1.0):\n",
    "    \"\"\"Function to train the neural network for one epoch with optional CutMix.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if np.random.rand() < cutmix_prob:\n",
    "            inputs, labels_a, labels_b, lam = cutmix(inputs, labels, alpha=cutmix_alpha)\n",
    "            outputs = model(inputs)\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "        total_labels += labels.size(0)\n",
    "        correct_labels += torch.sum(predicted_labels == labels).item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_labels / total_labels\n",
    "    \n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8fec0c9-e9e2-43db-8900-2806ffe825cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_nn(model, test_loader, criterion, device):\n",
    "    \"\"\"Function to evaluate the neural network on the test data.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += torch.sum(predicted_labels == labels).item()\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_labels / total_labels\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9cd4beb-0350-45b5-97d1-0daad9615b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_name, num_classes, feature_extractor=False):\n",
    "    \"\"\"\n",
    "    Prepare a model for training, either as a feature extractor or for fine-tuning.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained model\n",
    "    if model_name == \"resnet\":\n",
    "        model = models.resnet50(pretrained=True)\n",
    "    elif model_name == \"squeezenet\":\n",
    "        model = models.squeezenet1_1(pretrained=True)\n",
    "    elif model_name == \"convnext\":\n",
    "        model = models.convnext_base(pretrained=True)\n",
    "    elif model_name == \"vit\":\n",
    "        model = models.vit_b_16(pretrained=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "    \n",
    "    # Modify the last layer for the dataset\n",
    "    if model_name == \"squeezenet\":\n",
    "        model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "    elif model_name == \"convnext\":\n",
    "        model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n",
    "    elif model_name == \"vit\":\n",
    "        model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
    "    else:  # ResNet\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # If using as a fixed feature extractor, freeze all layers except the final layer\n",
    "    if feature_extractor:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Enable gradients only for the last layer\n",
    "        if model_name == \"squeezenet\":\n",
    "            for param in model.classifier[1].parameters():\n",
    "                param.requires_grad = True\n",
    "        elif model_name == \"convnext\":\n",
    "            for param in model.classifier[2].parameters():\n",
    "                param.requires_grad = True\n",
    "        elif model_name == \"vit\":\n",
    "            for param in model.heads.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:  # ResNet\n",
    "            for param in model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d891c78f-5f2a-4e13-840d-8ee7021a346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    lr,\n",
    "    num_epochs,\n",
    "    num_classes,\n",
    "    cutmix_alpha=1.0,\n",
    "    cutmix_prob=0.5,\n",
    "    weight_decay=1e-4,\n",
    "    log_dir=\"runs\",\n",
    "    save_path=\"saved_models\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run training for a model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model.\n",
    "        train_loader: DataLoader for training data.\n",
    "        test_loader: DataLoader for testing data.\n",
    "        lr (float): Learning rate.\n",
    "        num_epochs (int): Number of epochs.\n",
    "        num_classes (int): Number of classes in the dataset.\n",
    "        alpha (float): Mixup parameter.\n",
    "        cutmix_prob (float): Probability of applying CutMix.\n",
    "        weight_decay (float): Weight decay for optimizer.\n",
    "        log_dir (str): Directory for TensorBoard logs.\n",
    "        save_path (str): Directory to save the trained model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Prepare the model\n",
    "    model = prepare_model(model_name, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer and learning rate scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    train_losses, train_accuracies = [], []\n",
    "    test_losses, test_accuracies = [], []\n",
    "    learning_rates = []\n",
    "\n",
    "    # TensorBoard setup\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Training\n",
    "        train_loss, train_accuracy = train_nn(model, train_loader, criterion, optimizer, device, cutmix_alpha, cutmix_prob)\n",
    "        # Evaluation\n",
    "        test_loss, test_accuracy = evaluate_nn(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Test\", test_accuracy, epoch)\n",
    "        writer.add_scalar(\"Learning_Rate\", current_lr, epoch)\n",
    "\n",
    "        # Logging to console\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Save the model\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model_save_path = os.path.join(save_path, f\"{model_name}_final.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    return model, train_losses, train_accuracies, test_losses, test_accuracies, learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3729044b-0c3b-40bb-8662-fb39a00f38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_training(\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    lr,\n",
    "    fixed_num_epochs,\n",
    "    full_num_epochs,\n",
    "    num_classes,\n",
    "    freeze_feature_extractor=False,\n",
    "    full_finetune=False,\n",
    "    weight_decay=1e-4,\n",
    "    cutmix_alpha=1.0,\n",
    "    cutmix_prob=0.5,\n",
    "    log_dir=\"runs\",\n",
    "    save_path=\"saved_models\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Combined training function for using the model as a fixed feature extractor\n",
    "    and/or fully fine-tuning the model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model.\n",
    "        train_loader: DataLoader for training data.\n",
    "        test_loader: DataLoader for testing data.\n",
    "        lr (float): Learning rate.\n",
    "        num_epochs (int): Number of epochs.\n",
    "        num_classes (int): Number of classes in the dataset.\n",
    "        freeze_feature_extractor (bool): Whether to freeze feature extractor layers.\n",
    "        full_finetune (bool): Whether to fine-tune the entire model.\n",
    "        weight_decay (float): Weight decay for optimizer.\n",
    "        cutmix_alpha (float): Mixup parameter.\n",
    "        cutmix_prob (float): Probability of applying CutMix.\n",
    "        log_dir (str): Directory for TensorBoard logs.\n",
    "        save_path (str): Directory to save the trained model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Number of classes\n",
    "    num_classes = len(train_loader.dataset.classes)\n",
    "\n",
    "    # Prepare the model\n",
    "    model = prepare_model(model_name, num_classes)\n",
    "\n",
    "    # writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # If freeze_feature_extractor is enabled\n",
    "    if freeze_feature_extractor:\n",
    "        print(f\"Using {model_name} as a fixed feature extractor.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # # Unfreeze the classifier layer\n",
    "        # for param in model.fc.parameters():\n",
    "        #     param.requires_grad = True\n",
    "\n",
    "        num_logits = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_logits, num_classes)\n",
    "\n",
    "        # Training the model as a fixed feature extractor\n",
    "        # optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "        # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "        print(\"Training with fixed feature extractor...\")\n",
    "        start_time = time.time()\n",
    "        model, train_losses, train_accuracies, test_losses, test_accuracies, learning_rates = run_training(\n",
    "            model_name=model_name,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            lr=lr,\n",
    "            num_epochs=fixed_num_epochs,\n",
    "            num_classes=num_classes,\n",
    "            cutmix_alpha=cutmix_alpha,\n",
    "            cutmix_prob=cutmix_prob,\n",
    "            weight_decay=weight_decay,\n",
    "            log_dir=f\"{log_dir}/fixed_feature_extractor\",\n",
    "            save_path=f\"{save_path}/fixed_feature_extractor\",\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        # writer.add_scalar(\"Training Time/Fixed Feature Extractor\", elapsed_time)\n",
    "        print(f\"Fixed feature extractor training completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # If full_finetune is enabled\n",
    "    if full_finetune:\n",
    "        print(f\"Fine-tuning the entire {model_name} model.\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Training the model with full fine-tuning\n",
    "        # optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "        print(\"Training with full fine-tuning...\")\n",
    "        start_time = time.time()\n",
    "        model, train_losses, train_accuracies, test_losses, test_accuracies, learning_rates = run_training(\n",
    "            model_name=model_name,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            lr=lr,\n",
    "            num_epochs=full_num_epochs,\n",
    "            num_classes=num_classes,\n",
    "            cutmix_alpha=cutmix_alpha,\n",
    "            cutmix_prob=cutmix_prob,\n",
    "            weight_decay=weight_decay,\n",
    "            log_dir=f\"{log_dir}/full_finetune\",\n",
    "            save_path=f\"{save_path}/full_finetune\",\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        # writer.add_scalar(\"Training Time/Full Fine-Tuning\", elapsed_time)\n",
    "        print(f\"Full fine-tuning completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    # writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3c956ee-5cfc-4a08-bf6c-8f1a20221125",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/ahmad/courses/cuda_lab/MA-INF-4308-Lab-Vision-Systems/Assignment-3/dataset'\n",
    "# log_dir = './runs/human_robot_classifier_' + datetime.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e0e4608-25e5-4cc6-972c-dae1e25bd323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./runs/human_robot_classifier_20241118_032834\n"
     ]
    }
   ],
   "source": [
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f3df682-99a1-498d-bf88-bee004df9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# Set random seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85b179b1-cb2b-4752-9253-e8a32d593625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "    ], p=0.8),\n",
    "    # AutoAugment(AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c6bcf47-e3bd-4ef8-addd-973d5a6e5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f30e05d4-f037-4f14-9057-b76bffbf3d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['person', 'robot'], ['person', 'robot'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes, val_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3eda0564-a32b-4178-a0cc-77205b162751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e967ed-ecb5-47b3-b85f-cfb8750d7fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ahmad/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using resnet as a fixed feature extractor.\n",
      "Training with fixed feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████                                    | 1/5 [01:21<05:27, 81.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.7334, Train Accuracy: 0.6912\n",
      "Test Loss: 0.7313, Test Accuracy: 0.9000\n",
      "Learning Rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "combine_training(\n",
    "    model_name=\"resnet\",\n",
    "    train_loader=train_loader,\n",
    "    test_loader=val_loader,\n",
    "    lr=0.001,\n",
    "    fixed_num_epochs=5,\n",
    "    full_num_epochs=10,\n",
    "    num_classes=2,\n",
    "    freeze_feature_extractor=True,\n",
    "    full_finetune=False,\n",
    "    log_dir=\"logs\",\n",
    "    save_path=\"models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a5f4e-1da3-42d5-8da0-fe26a369bcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
