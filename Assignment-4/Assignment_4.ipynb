{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412bc820-04dc-44af-9fe4-428dbaf37b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d56308-69c9-4a49-be4b-f1cd428dc23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b263d2-3539-4c53-b757-48939f75f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9a314-f4a9-42de-b64d-8da159575c91",
   "metadata": {},
   "source": [
    "### LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae526f9-64fd-46ab-9f51-a87828ab85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Using\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Weights for input gate\n",
    "        self.Wi = nn.Parameter(torch.Tensor(hidden_size, input_size + hidden_size))\n",
    "        # Weights for forget gate\n",
    "        self.Wf = nn.Parameter(torch.Tensor(hidden_size, input_size + hidden_size))\n",
    "        # Weights for cell state\n",
    "        self.Wc = nn.Parameter(torch.Tensor(hidden_size, input_size + hidden_size))\n",
    "        # Weights for output gate\n",
    "        self.Wo = nn.Parameter(torch.Tensor(hidden_size, input_size + hidden_size))\n",
    "        \n",
    "        # Biases\n",
    "        if bias:\n",
    "            self.bi = nn.Parameter(torch.Tensor(hidden_size))\n",
    "            self.bf = nn.Parameter(torch.Tensor(hidden_size))\n",
    "            self.bc = nn.Parameter(torch.Tensor(hidden_size))\n",
    "            self.bo = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bi', None)\n",
    "            self.register_parameter('bf', None)\n",
    "            self.register_parameter('bc', None)\n",
    "            self.register_parameter('bo', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # Xavier/Glorot initialization\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for p in self.parameters():\n",
    "            p.data.uniform_(-std, std)\n",
    "    \n",
    "    def forward(self, x, state):\n",
    "        # Unpack previous hidden and cell states\n",
    "        h_prev, c_prev = state\n",
    "        \n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = torch.cat((x, h_prev), dim=1)\n",
    "        \n",
    "        # Input gate\n",
    "        i = torch.sigmoid(F.linear(combined, self.Wi, self.bi))\n",
    "        \n",
    "        # Forget gate\n",
    "        f = torch.sigmoid(F.linear(combined, self.Wf, self.bf))\n",
    "        \n",
    "        # Cell state candidate\n",
    "        c_candidate = torch.tanh(F.linear(combined, self.Wc, self.bc))\n",
    "        \n",
    "        # Output gate\n",
    "        o = torch.sigmoid(F.linear(combined, self.Wo, self.bo))\n",
    "        \n",
    "        # Update cell state\n",
    "        c_next = f * c_prev + i * c_candidate\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Create LSTM cells for each layer\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            LSTMCell(input_size if layer == 0 else hidden_size, \n",
    "                     hidden_size, bias) \n",
    "            for layer in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, states=None):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Initialize hidden states if not provided\n",
    "        if states is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "            states = (h0, c0)\n",
    "        \n",
    "        # Unpack initial states\n",
    "        h_prev, c_prev = states\n",
    "        \n",
    "        # Output tensor to store hidden states for each timestep\n",
    "        outputs = []\n",
    "        \n",
    "        # Process each timestep\n",
    "        for t in range(seq_len):\n",
    "            layer_input = x[:, t, :]\n",
    "            \n",
    "            # Process through each LSTM layer\n",
    "            layer_h_states = []\n",
    "            for layer in range(self.num_layers):\n",
    "                # Get cell state for this layer\n",
    "                h_prev_layer = h_prev[layer]\n",
    "                c_prev_layer = c_prev[layer]\n",
    "                \n",
    "                # Update hidden and cell states\n",
    "                h_next, c_next = self.lstm_cells[layer](layer_input, (h_prev_layer, c_prev_layer))\n",
    "                \n",
    "                # Update layer input for next layer\n",
    "                layer_input = h_next\n",
    "                \n",
    "                # Store updated states\n",
    "                layer_h_states.append(h_next)\n",
    "                h_prev[layer] = h_next\n",
    "                c_prev[layer] = c_next\n",
    "            \n",
    "            # Store final layer's hidden state\n",
    "            outputs.append(layer_h_states[-1])\n",
    "        \n",
    "        # Stack outputs and return\n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c341c1a-1c2a-4bf5-a0e7-7728823929a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTM(input_size\u001b[38;5;241m=\u001b[39m\u001b[43minput_size\u001b[49m, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size, num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print the model structure\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_size' is not defined"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "# Print the model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796a08e4-cf48-4401-a33b-91452aef092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 7, 5])\n"
     ]
    }
   ],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Create LSTM cells for each layer\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            nn.LSTMCell(input_size if i == 0 else hidden_size, hidden_size) \n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
    "        Returns:\n",
    "            output: Output tensor of shape (batch_size, seq_len, output_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Initialize hidden and cell states for each layer\n",
    "        h_t = [torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(self.num_layers)]\n",
    "        c_t = [torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        # To store the output at each time step\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Extract the time step t input\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Pass through each layer\n",
    "            for layer in range(self.num_layers):\n",
    "                h_t[layer], c_t[layer] = self.lstm_cells[layer](\n",
    "                    x_t, (h_t[layer], c_t[layer])\n",
    "                )\n",
    "                # The input to the next layer is the output of the current layer\n",
    "                x_t = h_t[layer]\n",
    "            \n",
    "            # Pass the final layer's output through the fully connected layer\n",
    "            output_t = self.fc(h_t[-1])\n",
    "            outputs.append(output_t)\n",
    "        \n",
    "        # Stack the outputs to form the final output tensor\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# Example usage\n",
    "input_size = 10  # Dimensionality of input features\n",
    "hidden_size = 20  # Number of hidden units\n",
    "output_size = 5  # Dimensionality of output features\n",
    "seq_len = 7  # Length of the input sequence\n",
    "batch_size = 3  # Batch size\n",
    "num_layers = 4  # Number of LSTM layers\n",
    "\n",
    "model = CustomLSTM(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# Random input tensor\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(\"Output shape:\", output.shape)  # Should be (batch_size, seq_len, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9f9e0b-ea1d-4190-ab75-9a52bfe70e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomLSTM(\n",
       "  (lstm_cells): ModuleList(\n",
       "    (0): LSTMCell(10, 20)\n",
       "    (1-3): 3 x LSTMCell(20, 20)\n",
       "  )\n",
       "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec247e-f8a3-44e5-b34b-a6cc0c3379ec",
   "metadata": {},
   "source": [
    "### ConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bed0f6-f323-4263-9ffb-2540ac0c9c2a",
   "metadata": {},
   "source": [
    "https://github.com/rogertrullo/pytorch_convlstm/blob/master/conv_lstm.py\n",
    "\n",
    "\n",
    "https://github.com/Atcold/pytorch-CortexNet/blob/master/model/ConvLSTMCell.py\n",
    "\n",
    "https://sladewinter.medium.com/video-frame-prediction-using-convlstm-network-in-pytorch-b5210a6ce582\n",
    "\n",
    "https://towardsdatascience.com/video-prediction-using-convlstm-with-pytorch-lightning-27b195fd21a2\n",
    "\n",
    "https://www.kaggle.com/code/nguyenmanhcuongg/pytorch-video-classification-with-conv2d-lstm/notebook\n",
    "\n",
    "https://www.kaggle.com/code/lonnieqin/video-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20de3cf8-45bb-4745-8bc8-5ca4bc3935d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim, kernel_size, padding=self.padding)\n",
    "    \n",
    "    def forward(self, x, state):\n",
    "        h_prev, c_prev = state\n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "        gates = self.conv(combined)\n",
    "        i, f, o, g = torch.chunk(gates, 4, dim=1)\n",
    "        i, f, o, g = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o), torch.tanh(g)\n",
    "        c = f * c_prev + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "        return h, (h, c)\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.cells = nn.ModuleList(\n",
    "            [ConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim, kernel_size) for i in range(num_layers)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, state=None):\n",
    "        b, t, c, h, w = x.size()\n",
    "        if state is None:\n",
    "            state = [(torch.zeros(b, self.cells[i].hidden_dim, h, w).to(x.device),\n",
    "                      torch.zeros(b, self.cells[i].hidden_dim, h, w).to(x.device)) for i in range(self.num_layers)]\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(t):\n",
    "            x_t = x[:, t]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                x_t, state[i] = cell(x_t, state[i])\n",
    "            outputs.append(x_t)\n",
    "        return torch.stack(outputs, dim=1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3071c8b2-80cf-4ace-8368-b2ce580fbcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM(\n",
      "  (cells): ModuleList(\n",
      "    (0): ConvLSTMCell(\n",
      "      (conv): Conv2d(67, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ConvLSTMCell(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Example ConvLSTM initialization\n",
    "input_dim = 3        # Number of input channels (e.g., RGB)\n",
    "hidden_dim = 64      # Number of hidden channels\n",
    "kernel_size = 3      # Size of the convolution kernel\n",
    "num_layers = 2       # Number of ConvLSTM layers\n",
    "\n",
    "model = ConvLSTM(input_dim, hidden_dim, kernel_size, num_layers)\n",
    "\n",
    "# Print the model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d92fef-49d7-458f-b2e4-2ae38d4458c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3179db4c-8c40-4f03-b20b-0384ac2234f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTHProcessedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        \n",
    "        # Gather all image paths and their corresponding labels\n",
    "        for label, category in enumerate(os.listdir(root_dir)):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            for img_file in os.listdir(category_path):\n",
    "                self.data.append((os.path.join(category_path, img_file), label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1750c82e-3def-4eac-8d14-385338905e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "dataset = KTHProcessedDataset(\n",
    "    root_dir=\"/home/nfs/inf6/data/datasets/kth_actions/processed\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce1c093-97dd-45c6-be4b-e117b47c5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f06e18f-3820-421e-8c27-35798ce14c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTHProcessedDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        # Traverse through action categories and their subfolders\n",
    "        for label, category in enumerate(os.listdir(root_dir)):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "            for subfolder in os.listdir(category_path):\n",
    "                subfolder_path = os.path.join(category_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path):\n",
    "                    frames = sorted(os.listdir(subfolder_path))  # Ensure frames are ordered\n",
    "                    if len(frames) >= sequence_length:\n",
    "                        self.data.append((subfolder_path, frames, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subfolder_path, frames, label = self.data[idx]\n",
    "\n",
    "        # Randomly select a starting point for the sequence\n",
    "        # start_idx = random.randint(0, len(frames) - self.sequence_length)\n",
    "        # selected_frames = frames[start_idx:start_idx + self.sequence_length]\n",
    "\n",
    "        # Select frames sequentially from the start, up to sequence_length\n",
    "        selected_frames = frames[:self.sequence_length]\n",
    "\n",
    "\n",
    "        # Load and transform the frames\n",
    "        sequence = []\n",
    "        for frame_file in selected_frames:\n",
    "            frame_path = os.path.join(subfolder_path, frame_file)\n",
    "            img = Image.open(frame_path).convert(\"L\")  # Convert to grayscale\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            # print(\"Image shape:\",  img.shape)\n",
    "            # Resize the image to 64x64 if needed\n",
    "            # img = img.resize((64, 64))  # Ensure size is 64x64\n",
    "            # Flatten the image to a 1D tensor\n",
    "            img = img.view(-1)  # Flattening the image to size 4096 (64x64)\n",
    "            sequence.append(img)\n",
    "\n",
    "        # Stack frames into a tensor of shape [seq_len, channels, height, width]\n",
    "        sequence = torch.stack(sequence, dim=0)\n",
    "        return sequence, label\n",
    "\n",
    "\n",
    "# Transformations and DataLoader\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomResizedCrop((64, 64)),  # Randomly crop and resize\n",
    "    # transforms.RandomHorizontalFlip(),       # Flip image horizontally\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81d95951-b0be-4d0a-863b-2a0d5c3c5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 40\n",
    "batch_size = 16\n",
    "\n",
    "dataset = KTHProcessedDataset(root_dir=\"/home/nfs/inf6/data/datasets/kth_actions/processed\", sequence_length=sequence_length, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True) # using drop_last because the last batch size is causing problems so using data with only full batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4f5929c-55e4-477f-80ea-ccef9b6fa27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e92ed59-e289-4e94-aea2-1d6a62761a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 537 kB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89122940-db0a-4931-8241-3ce6d9b177f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|███████████████████████████████████████████| 37/37 [00:22<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 58.5711, Accuracy: 23.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|███████████████████████████████████████████| 37/37 [00:23<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Loss: 53.8805, Accuracy: 27.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|███████████████████████████████████████████| 37/37 [00:21<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Loss: 53.6750, Accuracy: 28.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|███████████████████████████████████████████| 37/37 [00:22<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Loss: 50.0498, Accuracy: 30.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|███████████████████████████████████████████| 37/37 [00:21<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Loss: 48.5600, Accuracy: 29.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|███████████████████████████████████████████| 37/37 [00:22<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Loss: 50.5933, Accuracy: 31.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|███████████████████████████████████████████| 37/37 [00:22<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Loss: 48.3554, Accuracy: 33.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|███████████████████████████████████████████| 37/37 [00:22<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Loss: 51.0739, Accuracy: 28.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|███████████████████████████████████████████| 37/37 [00:23<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Loss: 50.6373, Accuracy: 34.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Loss: 48.1520, Accuracy: 35.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████████████████████████████████████| 37/37 [00:21<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Loss: 46.7283, Accuracy: 31.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Loss: 43.8926, Accuracy: 41.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Loss: 43.6088, Accuracy: 41.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Loss: 49.0614, Accuracy: 41.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Loss: 44.5216, Accuracy: 45.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████████████████████████████████████| 37/37 [00:23<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Loss: 42.3128, Accuracy: 44.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Loss: 41.1004, Accuracy: 46.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Loss: 41.1744, Accuracy: 45.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████████████████████████████████████| 37/37 [00:21<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Loss: 39.3515, Accuracy: 46.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████████████████████████████████████| 37/37 [00:21<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Loss: 40.5433, Accuracy: 46.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████████████████████████████████████| 37/37 [00:21<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Loss: 40.5336, Accuracy: 46.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Loss: 36.7278, Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Loss: 34.6514, Accuracy: 51.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████████████████████████████████████| 37/37 [00:21<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Loss: 36.9132, Accuracy: 52.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████████████████████████████████████| 37/37 [00:22<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Loss: 36.5339, Accuracy: 53.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the CustomLSTM model\n",
    "input_size = 64 * 64  # Flattened image size (for grayscale images)\n",
    "hidden_size = 256\n",
    "output_size = len(os.listdir(\"/home/nfs/inf6/data/datasets/kth_actions/processed\"))  # Number of action classes\n",
    "num_layers = 4\n",
    "\n",
    "model = CustomLSTM(input_size, hidden_size, output_size, num_layers)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # print(\"Seq shape: \", sequences.shape)\n",
    "        # Prepare input and labels\n",
    "        sequences = sequences.view(batch_size, sequence_length, -1).to(device)  # Flatten each frame\n",
    "        labels = labels.to(device)\n",
    "        # print(sequences.shape)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)  # Shape: [batch_size, seq_len, output_size]\n",
    "        outputs = outputs[:, -1, :]  # Use the output of the last time step for classification\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165d583-c5e6-4c81-ba75-0c0ed5eca7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
