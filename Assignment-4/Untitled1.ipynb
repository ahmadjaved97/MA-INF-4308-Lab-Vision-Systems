{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7e125e-3911-4243-b96d-87421747a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5974654-69ed-4716-bf2a-9cd8dd2e9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "323bfd1f-f625-483f-acf6-68df54404104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTHProcessedDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.class_mapping = {}  # To store class ID to name mapping\n",
    "\n",
    "        # Traverse through action categories and their subfolders\n",
    "        for label, category in enumerate(sorted(os.listdir(root_dir))):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "            self.class_mapping[label] = category  # Map class ID to category name\n",
    "            for subfolder in os.listdir(category_path):\n",
    "                subfolder_path = os.path.join(category_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path):\n",
    "                    frames = sorted(os.listdir(subfolder_path))  # Ensure frames are ordered\n",
    "\n",
    "                    # Check if there are enough frames\n",
    "                    if len(frames) >= sequence_length:\n",
    "                        self.data.append((subfolder_path, frames, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subfolder_path, frames, label = self.data[idx]\n",
    "\n",
    "        # Select frames sequentially from the start, up to sequence_length\n",
    "        selected_frames = frames[:self.sequence_length]\n",
    "\n",
    "        sequence = []\n",
    "        for frame_file in selected_frames:\n",
    "            frame_path = os.path.join(subfolder_path, frame_file)\n",
    "            try:\n",
    "                # Try to open the image\n",
    "                img = Image.open(frame_path).convert(\"L\")  # Convert to grayscale\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "\n",
    "                # Flatten the image to a 1D tensor\n",
    "                img = img.view(-1)  # Flattening the image to size 4096 (64x64)\n",
    "                sequence.append(img)\n",
    "\n",
    "            except (IOError, OSError) as e:\n",
    "                # Log the error and skip the corrupted frame\n",
    "                print(f\"Warning: Skipping corrupted image {frame_path} due to error: {e}\")\n",
    "                return self.__getitem__((idx + 1) % len(self))  # Skip to the next sample\n",
    "\n",
    "        # Stack frames into a tensor of shape [sequence_length, 4096]\n",
    "        sequence = torch.stack(sequence, dim=0)\n",
    "        return sequence, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a777ac4-2afc-41b5-8853-45c32cf7dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentations and transformations\n",
    "transform = transforms.Compose([\n",
    "    # Spatial augmentations\n",
    "    # RandomHorizontalFlip(p=0.5),                # Flip frames horizontally with 50% probability\n",
    "    # RandomRotation(degrees=15),                # Random rotation within ±15 degrees\n",
    "    transforms.RandomCrop(size=(64, 64), pad_if_needed=True),  # Random crop to 64x64, pad if needed\n",
    "\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2)], p=0.3),  # Adjust brightness/contrast\n",
    "    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),  # Random blur\n",
    "    \n",
    "    # Conversion and normalization\n",
    "    transforms.ToTensor(),                                # Convert PIL image to tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])           # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "352907cd-6e53-4d80-8f21-f07a614d705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and hyperparameters\n",
    "root_dir = '/home/nfs/inf6/data/datasets/kth_actions/processed'\n",
    "sequence_length = 45\n",
    "batch_size = 16\n",
    "train_ratio = 0.8  # 80% for training, 20% for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cfee72-3eb0-4117-954a-ee879f1d3c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 599\n",
      "Train set size: 479\n",
      "Validation set size: 120\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "dataset = KTHProcessedDataset(root_dir=root_dir, sequence_length=sequence_length, transform=transform)\n",
    "\n",
    "# Calculate train and validation sizes\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for train and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "print(f\"Train set size: {train_size}\")\n",
    "print(f\"Validation set size: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d036d7-f23f-44d0-8ae7-118f4899f8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Classes:\n",
      "Class ID: 0, Class Name: boxing\n",
      "Class ID: 1, Class Name: handclapping\n",
      "Class ID: 2, Class Name: handwaving\n",
      "Class ID: 3, Class Name: jogging\n",
      "Class ID: 4, Class Name: running\n",
      "Class ID: 5, Class Name: walking\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Classes:\")\n",
    "for class_id, class_name in dataset.class_mapping.items():\n",
    "    print(f\"Class ID: {class_id}, Class Name: {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f568961f-89f5-4c29-b025-f17c1e41da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the LSTM model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(-1, 45, 1, height, width)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # Get outputs for the full sequence\n",
    "        \n",
    "        # Only use the last time step's output for classification\n",
    "        loss = criterion(outputs, labels)  # Use the last timestep output\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy for the last time step\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        correct_labels += (predicted_labels == labels).sum().item()\n",
    "        total_labels += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct_labels / total_labels\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_nn(model, eval_loader, criterion, device):\n",
    "    \"\"\"Evaluate the LSTM model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in eval_loader:\n",
    "            inputs = inputs.view(-1, 45, 1, height, width)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)  # Get outputs for the full sequence\n",
    "            \n",
    "            loss = criterion(outputs, labels)  # Use the last timestep output\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy for the last time step\n",
    "            _, predicted_labels = torch.max(outputs, dim=1)\n",
    "            correct_labels += (predicted_labels == labels).sum().item()\n",
    "            total_labels += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(eval_loader)\n",
    "    accuracy = correct_labels / total_labels\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def run_training(model, train_loader, eval_loader, criterion, device, num_epochs, learning_rate=0.001, step_size=5, gamma=0.5,\n",
    "                 project='lstm_training', name='test_run'):\n",
    "    \"\"\"Train and evaluate the LSTM model for a given number of epochs with W&B logging\"\"\"\n",
    "    # Initialize W&B logging\n",
    "    wandb.init(project=project, name=name, config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"step_size\": step_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"optimizer\": \"Adam\",\n",
    "    },\n",
    "    )\n",
    "    config = wandb.config\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)  # Learning rate scheduler\n",
    "    \n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    train_accuracies = []\n",
    "    eval_accuracies = []\n",
    "    learning_rates = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Train for one epoch\n",
    "        train_loss, train_accuracy = train_nn(model, train_loader, criterion, optimizer, device)\n",
    "        # Evaluate after each epoch\n",
    "        eval_loss, eval_accuracy = evaluate_nn(model, eval_loader, criterion, device)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        # Record the metrics\n",
    "        train_losses.append(train_loss)\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        eval_accuracies.append(eval_accuracy)\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"eval_accuracy\": eval_accuracy,\n",
    "            \"learning_rate\": current_lr\n",
    "        })\n",
    "\n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    # Finish W&B run\n",
    "    wandb.finish()\n",
    "\n",
    "    return model, train_losses, train_accuracies, eval_losses, eval_accuracies, learning_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46abed5-eef1-441d-b97d-9a1ca936deb5",
   "metadata": {},
   "source": [
    "### Pytorch LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4ead54-c601-4d67-9943-ca32b94c7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Create LSTM cells for each layer\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            nn.LSTMCell(input_size if i == 0 else hidden_size, hidden_size) \n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
    "        Returns:\n",
    "            output: Output tensor of shape (batch_size, seq_len, output_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Initialize hidden and cell states for each layer\n",
    "        h_t = [torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(self.num_layers)]\n",
    "        c_t = [torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        # To store the output at each time step\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Extract the time step t input\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Pass through each layer\n",
    "            for layer in range(self.num_layers):\n",
    "                h_t[layer], c_t[layer] = self.lstm_cells[layer](\n",
    "                    x_t, (h_t[layer], c_t[layer])\n",
    "                )\n",
    "                # The input to the next layer is the output of the current layer\n",
    "                x_t = h_t[layer]\n",
    "            \n",
    "            # Pass the final layer's output through the fully connected layer\n",
    "            output_t = self.fc(h_t[-1])\n",
    "            outputs.append(output_t)\n",
    "        \n",
    "        # Stack the outputs to form the final output tensor\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee0f8ee4-0115-40c9-bfdf-4126bbc4e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexActionRecognitionModel(nn.Module):\n",
    "    def __init__(self, sequence_length, input_dim=64*64, num_classes=6):\n",
    "        super(ComplexActionRecognitionModel, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Convolutional Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (64, 32, 32)\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (128, 16, 16)\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Output: (256, 16, 16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Output: (256, 16, 16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (256, 8, 8)\n",
    "            nn.Dropout(0.4),\n",
    "        )\n",
    "\n",
    "        # Replace PyTorch LSTM with Custom LSTM\n",
    "        self.rnn = CustomLSTM(\n",
    "            input_size=256 * 8 * 8,  # Flattened size from encoder\n",
    "            hidden_size=256,         # Hidden state size\n",
    "            output_size=256,         # Output size per time step\n",
    "            num_layers=2             # Number of LSTM layers\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=1),  # Temporal conv\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Reduce sequence length to 1\n",
    "            nn.Flatten(),  # Flatten the output\n",
    "            nn.Linear(128, num_classes)  # Final classification layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, 64*64]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Reshape and encode each frame\n",
    "        x = x.view(batch_size * seq_len, 1, 64, 64)  # Reshape to (batch_size * sequence_length, 1, 64, 64)\n",
    "        x = self.encoder(x)  # Pass through convolutional encoder\n",
    "        x = x.view(batch_size, seq_len, -1)  # Reshape to (batch_size, sequence_length, 256*8*8)\n",
    "        \n",
    "        # Recurrent Module\n",
    "        x = self.rnn(x)  # Custom LSTM output shape: (batch_size, sequence_length, 256)\n",
    "        \n",
    "        # Classifier\n",
    "        x = x.transpose(1, 2)  # Change shape to (batch_size, 256, sequence_length) for Conv1d\n",
    "        x = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9493f3d-2902-4bfb-a31c-18ff353e43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "sequence_length = 45\n",
    "input_dim = 64 * 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "step_size = 5\n",
    "gamma = 0.5\n",
    "\n",
    "# Define the criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2db787e-e5f1-4aaf-a4e9-56d945c872c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplexActionRecognitionModel(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout(p=0.3, inplace=False)\n",
       "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU()\n",
       "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (rnn): CustomLSTM(\n",
       "    (lstm_cells): ModuleList(\n",
       "      (0): LSTMCell(16384, 256)\n",
       "      (1): LSTMCell(256, 256)\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): AdaptiveAvgPool1d(output_size=1)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ComplexActionRecognitionModel(input_dim=input_dim, sequence_length=sequence_length, num_classes=num_classes).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1f0d47-91aa-4bc9-a801-b4e103c20fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahmadjaved97\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/javeda1/MA-INF-4308-Lab-Vision-Systems/Assignment-4/wandb/run-20241130_160002-oeir74r3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/oeir74r3' target=\"_blank\">pth_lstm</a></strong> to <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/oeir74r3' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/oeir74r3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▌                                                  | 1/10 [00:17<02:39, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.3945, Train Accuracy: 0.3215\n",
      "Eval Loss: 1.2967, Eval Accuracy: 0.3333\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████▏                                            | 2/10 [00:32<02:07, 15.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 1.1502, Train Accuracy: 0.3737\n",
      "Eval Loss: 1.0825, Eval Accuracy: 0.4417\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████▊                                       | 3/10 [00:47<01:48, 15.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 1.1374, Train Accuracy: 0.4363\n",
      "Eval Loss: 1.0816, Eval Accuracy: 0.4250\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████▍                                 | 4/10 [01:02<01:31, 15.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 1.1175, Train Accuracy: 0.3758\n",
      "Eval Loss: 0.9975, Eval Accuracy: 0.4667\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████                            | 5/10 [01:17<01:15, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 1.0168, Train Accuracy: 0.4593\n",
      "Eval Loss: 0.9421, Eval Accuracy: 0.4417\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████▌                      | 6/10 [01:32<01:00, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 0.9499, Train Accuracy: 0.4843\n",
      "Eval Loss: 0.8649, Eval Accuracy: 0.5667\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████▏                | 7/10 [01:46<00:44, 14.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 0.8751, Train Accuracy: 0.5678\n",
      "Eval Loss: 0.7853, Eval Accuracy: 0.5917\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████▊           | 8/10 [02:01<00:29, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 0.8598, Train Accuracy: 0.5637\n",
      "Eval Loss: 0.7806, Eval Accuracy: 0.6000\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████▍     | 9/10 [02:16<00:14, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 0.7797, Train Accuracy: 0.6326\n",
      "Eval Loss: 0.7810, Eval Accuracy: 0.6583\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 10/10 [02:31<00:00, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 0.8333, Train Accuracy: 0.5866\n",
      "Eval Loss: 0.7435, Eval Accuracy: 0.6000\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_accuracy</td><td>▁▃▃▄▃▆▇▇█▇</td></tr><tr><td>eval_loss</td><td>█▅▅▄▄▃▂▁▁▁</td></tr><tr><td>learning_rate</td><td>█████▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▄▂▄▅▇▆█▇</td></tr><tr><td>train_loss</td><td>█▅▅▅▄▃▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_accuracy</td><td>0.6</td></tr><tr><td>eval_loss</td><td>0.74352</td></tr><tr><td>learning_rate</td><td>0.0005</td></tr><tr><td>train_accuracy</td><td>0.58664</td></tr><tr><td>train_loss</td><td>0.83326</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pth_lstm</strong> at: <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/oeir74r3' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/oeir74r3</a><br/> View project at: <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241130_160002-oeir74r3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run training\n",
    "model, train_losses, train_accuracies, eval_losses, eval_accuracies, learning_rates = run_training(\n",
    "    model, train_loader, val_loader, criterion, device,\n",
    "    num_epochs=num_epochs, learning_rate=learning_rate, step_size=step_size, gamma=gamma, name='pth_lstm', project='assignment_4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e6ee4-9050-4caa-acd8-e895b263ec45",
   "metadata": {},
   "source": [
    "### Pytorch GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "144934e2-7e00-4fc0-919a-6f98bc8d8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(CustomGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Create GRU cells for each layer\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_size if i == 0 else hidden_size, hidden_size)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
    "        Returns:\n",
    "            output: Output tensor of shape (batch_size, seq_len, output_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Initialize hidden states for each layer\n",
    "        h_t = [torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        # To store the output at each time step\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Extract the time step t input\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Pass through each layer\n",
    "            for layer in range(self.num_layers):\n",
    "                h_t[layer] = self.gru_cells[layer](x_t, h_t[layer])\n",
    "                # The input to the next layer is the output of the current layer\n",
    "                x_t = h_t[layer]\n",
    "            \n",
    "            # Pass the final layer's output through the fully connected layer\n",
    "            output_t = self.fc(h_t[-1])\n",
    "            outputs.append(output_t)\n",
    "        \n",
    "        # Stack the outputs to form the final output tensor\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5428c6f3-afef-4f77-a08f-56b0a1db6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexActionRecognitionModelGRU(nn.Module):\n",
    "    def __init__(self, sequence_length, input_dim=64*64, num_classes=6):\n",
    "        super(ComplexActionRecognitionModelGRU, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Convolutional Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (64, 32, 32)\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (128, 16, 16)\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Output: (256, 16, 16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Output: (256, 16, 16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (256, 8, 8)\n",
    "            nn.Dropout(0.4),\n",
    "        )\n",
    "\n",
    "        # Replace PyTorch LSTM with Custom GRU\n",
    "        self.rnn = CustomGRU(\n",
    "            input_size=256 * 8 * 8,  # Flattened size from encoder\n",
    "            hidden_size=256,         # Hidden state size\n",
    "            output_size=256,         # Output size per time step\n",
    "            num_layers=2             # Number of GRU layers\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=1),  # Temporal conv\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Reduce sequence length to 1\n",
    "            nn.Flatten(),  # Flatten the output\n",
    "            nn.Linear(128, num_classes)  # Final classification layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, 64*64]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Reshape and encode each frame\n",
    "        x = x.view(batch_size * seq_len, 1, 64, 64)  # Reshape to (batch_size * sequence_length, 1, 64, 64)\n",
    "        x = self.encoder(x)  # Pass through convolutional encoder\n",
    "        x = x.view(batch_size, seq_len, -1)  # Reshape to (batch_size, sequence_length, 256*8*8)\n",
    "        \n",
    "        # Recurrent Module\n",
    "        x = self.rnn(x)  # Custom GRU output shape: (batch_size, sequence_length, 256)\n",
    "        \n",
    "        # Classifier\n",
    "        x = x.transpose(1, 2)  # Change shape to (batch_size, 256, sequence_length) for Conv1d\n",
    "        x = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c463b15-83e8-49cb-8e20-7a37c5953d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplexActionRecognitionModelGRU(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout(p=0.3, inplace=False)\n",
       "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU()\n",
       "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (rnn): CustomGRU(\n",
       "    (gru_cells): ModuleList(\n",
       "      (0): GRUCell(16384, 256)\n",
       "      (1): GRUCell(256, 256)\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): AdaptiveAvgPool1d(output_size=1)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ComplexActionRecognitionModelGRU(input_dim=input_dim, sequence_length=sequence_length, num_classes=num_classes).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a299296-ed5a-442c-8446-96c106d91719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vknq43sy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pth_lstm</strong> at: <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/vknq43sy' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/vknq43sy</a><br/> View project at: <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241130_160714-vknq43sy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vknq43sy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/javeda1/MA-INF-4308-Lab-Vision-Systems/Assignment-4/wandb/run-20241130_160727-5tcgiruc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/5tcgiruc' target=\"_blank\">pth_GRU</a></strong> to <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/5tcgiruc' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/5tcgiruc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▌                                                  | 1/10 [00:14<02:11, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.2009, Train Accuracy: 0.3820\n",
      "Eval Loss: 1.1400, Eval Accuracy: 0.3833\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████▏                                            | 2/10 [00:29<01:56, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 1.0525, Train Accuracy: 0.4405\n",
      "Eval Loss: 0.9315, Eval Accuracy: 0.5333\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████▊                                       | 3/10 [00:43<01:42, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 0.9382, Train Accuracy: 0.5219\n",
      "Eval Loss: 0.8583, Eval Accuracy: 0.5750\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████▍                                 | 4/10 [00:58<01:28, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 0.8790, Train Accuracy: 0.5720\n",
      "Eval Loss: 0.7472, Eval Accuracy: 0.6083\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████                            | 5/10 [01:13<01:13, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 0.7939, Train Accuracy: 0.6221\n",
      "Eval Loss: 0.7250, Eval Accuracy: 0.6333\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████▌                      | 6/10 [01:28<00:58, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 0.6489, Train Accuracy: 0.6848\n",
      "Eval Loss: 0.6460, Eval Accuracy: 0.7333\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████▏                | 7/10 [01:42<00:44, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 0.5795, Train Accuracy: 0.7035\n",
      "Eval Loss: 0.6520, Eval Accuracy: 0.7083\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████▊           | 8/10 [01:57<00:29, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 0.6026, Train Accuracy: 0.7015\n",
      "Eval Loss: 0.5555, Eval Accuracy: 0.7083\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████▍     | 9/10 [02:12<00:14, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 0.5262, Train Accuracy: 0.7370\n",
      "Eval Loss: 0.5993, Eval Accuracy: 0.6917\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 10/10 [02:26<00:00, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 0.5116, Train Accuracy: 0.7599\n",
      "Eval Loss: 0.6588, Eval Accuracy: 0.7000\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_accuracy</td><td>▁▄▅▆▆█▇▇▇▇</td></tr><tr><td>eval_loss</td><td>█▆▅▃▃▂▂▁▂▂</td></tr><tr><td>learning_rate</td><td>█████▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▄▅▅▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_accuracy</td><td>0.7</td></tr><tr><td>eval_loss</td><td>0.65878</td></tr><tr><td>learning_rate</td><td>0.0005</td></tr><tr><td>train_accuracy</td><td>0.75992</td></tr><tr><td>train_loss</td><td>0.5116</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pth_GRU</strong> at: <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/5tcgiruc' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/5tcgiruc</a><br/> View project at: <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241130_160727-5tcgiruc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run training\n",
    "model, train_losses, train_accuracies, eval_losses, eval_accuracies, learning_rates = run_training(\n",
    "    model, train_loader, val_loader, criterion, device,\n",
    "    num_epochs=num_epochs, learning_rate=learning_rate, step_size=step_size, gamma=gamma, name='pth_GRU', project='assignment_4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa843b-b167-4e92-abc3-da40d442dbce",
   "metadata": {},
   "source": [
    "### Own LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d32fcce-b29e-4cd9-bd4d-559267709df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom LSTM Cell implementation from scratch\n",
    "    \n",
    "    Args:\n",
    "    - input_size: Number of input features\n",
    "    - hidden_size: Number of hidden units\n",
    "    - bias: Whether to use bias terms (default: True)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Combine all gates' weights into a single matrix for efficiency\n",
    "        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n",
    "        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n",
    "        \n",
    "        # Bias terms\n",
    "        if bias:\n",
    "            self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))\n",
    "            self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Xavier uniform initialization for weights\n",
    "        \"\"\"\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -std, std)\n",
    "    \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Forward pass for LSTM Cell\n",
    "        \n",
    "        Args:\n",
    "        - x: input tensor of shape (batch_size, input_size)\n",
    "        - hidden_state: tuple of (h, c) - previous hidden and cell states\n",
    "        \n",
    "        Returns:\n",
    "        - new_h: new hidden state\n",
    "        - new_c: new cell state\n",
    "        \"\"\"\n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden_state is None:\n",
    "            batch_size = x.size(0)\n",
    "            h = x.new_zeros(batch_size, self.hidden_size)\n",
    "            c = x.new_zeros(batch_size, self.hidden_size)\n",
    "        else:\n",
    "            h, c = hidden_state\n",
    "        \n",
    "        # Compute gate inputs\n",
    "        gates = F.linear(x, self.weight_ih, self.bias_ih) + \\\n",
    "                F.linear(h, self.weight_hh, self.bias_hh)\n",
    "        \n",
    "        # Split gates\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "        \n",
    "        # Apply activation functions\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "        \n",
    "        # Update cell state\n",
    "        new_c = (forgetgate * c) + (ingate * cellgate)\n",
    "        \n",
    "        # Compute new hidden state\n",
    "        new_h = outgate * torch.tanh(new_c)\n",
    "        \n",
    "        return new_h, new_c\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Full LSTM implementation using LSTMCell\n",
    "    \n",
    "    Args:\n",
    "    - input_size: Number of input features\n",
    "    - hidden_size: Number of hidden units\n",
    "    - num_layers: Number of LSTM layers\n",
    "    - num_classes: Number of output classes\n",
    "    - batch_first: Whether input is batch first (default: True)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, batch_first=True):\n",
    "        print(\"here it is\")\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        # Create LSTM cells for each layer\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            LSTMCell(input_size if i == 0 else hidden_size, hidden_size)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-layer LSTM\n",
    "        \n",
    "        Args:\n",
    "        - x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        - output: LSTM outputs for all time steps\n",
    "        \"\"\"\n",
    "        # Ensure input is batch first\n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Initialize hidden states for all layers\n",
    "        h_list = [torch.zeros(batch_size, self.hidden_size, device=x.device) \n",
    "                  for _ in range(self.num_layers)]\n",
    "        c_list = [torch.zeros(batch_size, self.hidden_size, device=x.device) \n",
    "                  for _ in range(self.num_layers)]\n",
    "        \n",
    "        # Collect outputs for all time steps\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            input_t = x[:, t, :]\n",
    "            \n",
    "            for layer in range(self.num_layers):\n",
    "                cell = self.lstm_cells[layer]\n",
    "                h_prev = input_t if layer == 0 else h_list[layer - 1]\n",
    "                h_list[layer], c_list[layer] = cell(h_prev, (h_list[layer], c_list[layer]))\n",
    "            \n",
    "            # Append the output of the final layer at time t\n",
    "            outputs.append(h_list[-1])\n",
    "        \n",
    "        # Stack outputs to form the full sequence\n",
    "        outputs = torch.stack(outputs, dim=1)  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e406c4ee-2f68-4012-b7f1-ff386661cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexActionRecognitionModelLSTMO(nn.Module):\n",
    "    def __init__(self, sequence_length, input_dim=64*64, num_classes=6):\n",
    "        super(ComplexActionRecognitionModelLSTMO, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Convolutional Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (64, 32, 32)\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (128, 16, 16)\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Output: (256, 16, 16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Output: (256, 16, 16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (256, 8, 8)\n",
    "            nn.Dropout(0.4),\n",
    "        )\n",
    "\n",
    "        # Custom LSTM Module\n",
    "        self.rnn = LSTM(\n",
    "            input_size=256 * 8 * 8,  # Flattened size from encoder\n",
    "            hidden_size=256,         # Hidden state size\n",
    "            num_layers=2,            # Number of LSTM layers\n",
    "            num_classes=256,         # Output size per time step (matches hidden size)\n",
    "            batch_first=True         # Input format: (batch_size, seq_len, input_size)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=1),  # Temporal conv\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Reduce sequence length to 1\n",
    "            nn.Flatten(),  # Flatten the output\n",
    "            nn.Linear(128, num_classes)  # Final classification layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Reshape and encode each frame\n",
    "        x = x.view(batch_size * seq_len, 1, 64, 64)  # Reshape to (batch_size * sequence_length, 1, 64, 64)\n",
    "        x = self.encoder(x)  # Pass through convolutional encoder\n",
    "        x = x.view(batch_size, seq_len, -1)  # Reshape to (batch_size, sequence_length, 256*8*8)\n",
    "        \n",
    "        # Recurrent Module\n",
    "        x = self.rnn(x)  # Custom LSTM output shape: (batch_size, sequence_length, 256)\n",
    "        \n",
    "        # Verify shape before transpose\n",
    "        # print(\"Post-RNN Shape:\", x.shape)  # Add this debug statement\n",
    "        \n",
    "        # Classifier\n",
    "        x = x.transpose(1, 2)  # Change shape to (batch_size, 256, sequence_length) for Conv1d\n",
    "        x = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a913ce40-f8e8-436b-85c1-bc54faa5be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here it is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ComplexActionRecognitionModelLSTMO(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout(p=0.3, inplace=False)\n",
       "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU()\n",
       "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (rnn): LSTM(\n",
       "    (lstm_cells): ModuleList(\n",
       "      (0-1): 2 x LSTMCell()\n",
       "    )\n",
       "    (classifier): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): AdaptiveAvgPool1d(output_size=1)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ComplexActionRecognitionModelLSTMO(input_dim=input_dim, sequence_length=sequence_length, num_classes=num_classes).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a94fe8c-ab0d-466c-96b2-2b3780c93ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahmadjaved97\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/javeda1/MA-INF-4308-Lab-Vision-Systems/Assignment-4/wandb/run-20241130_162512-jk5a0qul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/jk5a0qul' target=\"_blank\">own_lstm</a></strong> to <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/jk5a0qul' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/jk5a0qul</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▌                                                  | 1/10 [00:17<02:41, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.3967, Train Accuracy: 0.2985\n",
      "Eval Loss: 1.2756, Eval Accuracy: 0.3417\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████▏                                            | 2/10 [00:32<02:09, 16.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 1.1504, Train Accuracy: 0.3236\n",
      "Eval Loss: 1.2039, Eval Accuracy: 0.3583\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████▊                                       | 3/10 [00:47<01:48, 15.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 1.1103, Train Accuracy: 0.3716\n",
      "Eval Loss: 1.0655, Eval Accuracy: 0.3833\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████▍                                 | 4/10 [01:02<01:32, 15.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 1.0656, Train Accuracy: 0.4050\n",
      "Eval Loss: 1.0343, Eval Accuracy: 0.4833\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████                            | 5/10 [01:18<01:17, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 0.9926, Train Accuracy: 0.4447\n",
      "Eval Loss: 0.9737, Eval Accuracy: 0.4750\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████▌                      | 6/10 [01:33<01:01, 15.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 0.9628, Train Accuracy: 0.5010\n",
      "Eval Loss: 0.9079, Eval Accuracy: 0.5167\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████▏                | 7/10 [01:48<00:45, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 0.9033, Train Accuracy: 0.5470\n",
      "Eval Loss: 0.9197, Eval Accuracy: 0.5250\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████▊           | 8/10 [02:03<00:30, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 0.8658, Train Accuracy: 0.5846\n",
      "Eval Loss: 0.8267, Eval Accuracy: 0.5500\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████▍     | 9/10 [02:18<00:15, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 0.8296, Train Accuracy: 0.6096\n",
      "Eval Loss: 0.8568, Eval Accuracy: 0.5667\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 10/10 [02:33<00:00, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 0.7501, Train Accuracy: 0.6618\n",
      "Eval Loss: 0.7451, Eval Accuracy: 0.6250\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_accuracy</td><td>▁▁▂▄▄▅▆▆▇█</td></tr><tr><td>eval_loss</td><td>█▇▅▅▄▃▃▂▂▁</td></tr><tr><td>learning_rate</td><td>█████▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▂▃▄▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▅▅▄▄▃▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_accuracy</td><td>0.625</td></tr><tr><td>eval_loss</td><td>0.74508</td></tr><tr><td>learning_rate</td><td>0.0005</td></tr><tr><td>train_accuracy</td><td>0.6618</td></tr><tr><td>train_loss</td><td>0.75011</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">own_lstm</strong> at: <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/jk5a0qul' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/jk5a0qul</a><br/> View project at: <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241130_162512-jk5a0qul/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run training\n",
    "model, train_losses, train_accuracies, eval_losses, eval_accuracies, learning_rates = run_training(\n",
    "    model, train_loader, val_loader, criterion, device,\n",
    "    num_epochs=num_epochs, learning_rate=learning_rate, step_size=step_size, gamma=gamma, name='own_lstm', project='assignment_4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7583ae6d-f0e8-495a-a347-c080e6fa54cd",
   "metadata": {},
   "source": [
    "### Own ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7465510-f561-4199-b608-9739c3ffed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        # Gates: i, f, o, g\n",
    "        self.conv = nn.Conv2d(\n",
    "            input_channels + hidden_channels, \n",
    "            4 * hidden_channels, \n",
    "            kernel_size, \n",
    "            padding=self.padding, \n",
    "            bias=self.bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        # Concatenate along channel dimension\n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "        conv_output = self.conv(combined)\n",
    "\n",
    "        # Split into gates\n",
    "        i, f, o, g = torch.split(conv_output, self.hidden_channels, dim=1)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        # Update cell state and hidden state\n",
    "        c_next = f * c_prev + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, height, width):\n",
    "        h = torch.zeros(batch_size, self.hidden_channels, height, width, device=self.conv.weight.device)\n",
    "        c = torch.zeros(batch_size, self.hidden_channels, height, width, device=self.conv.weight.device)\n",
    "        return h, c\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, num_layers, num_classes, height, width, bias=True):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        # ConvLSTM layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            ConvLSTMCell(\n",
    "                input_channels=input_channels if i == 0 else hidden_channels[i - 1],\n",
    "                hidden_channels=hidden_channels[i],\n",
    "                kernel_size=kernel_size,\n",
    "                bias=bias\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_channels[-1] * height * width, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, sequence_length, channels, height, width]\n",
    "        Returns logits for classification.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _, height, width = x.size()\n",
    "        assert height == self.height and width == self.width, \"Input size mismatch with initialized height and width.\"\n",
    "\n",
    "        # Initialize hidden states for all layers\n",
    "        hidden_states = [layer.init_hidden(batch_size, height, width) for layer in self.layers]\n",
    "\n",
    "        # Process sequence through ConvLSTM layers\n",
    "        for t in range(seq_len):\n",
    "            current_input = x[:, t]\n",
    "            for l, layer in enumerate(self.layers):\n",
    "                hidden_states[l] = layer(current_input, hidden_states[l])\n",
    "                current_input = hidden_states[l][0]  # Use the hidden state (h_t)\n",
    "\n",
    "        # Last layer's hidden state at the last time step\n",
    "        last_hidden_state = hidden_states[-1][0]  # Shape: [batch_size, hidden_channels[-1], height, width]\n",
    "        return last_hidden_state  # Return the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568bd767-f788-4c2f-beff-8de952aacb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexActionRecognitionModelConvLSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ComplexActionRecognitionModelConvLSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Convolutional Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (64, 32, 32)\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (128, 16, 16)\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Output: (256, 16, 16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Output: (256, 16, 16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (256, 8, 8)\n",
    "            nn.Dropout(0.4),\n",
    "        )\n",
    "\n",
    "        # ConvLSTM\n",
    "        self.convlstm = ConvLSTM(\n",
    "            input_channels=256,\n",
    "            hidden_channels=[64, 64],  # Two layers with 64 channels each\n",
    "            kernel_size=3,\n",
    "            num_layers=2,\n",
    "            num_classes=num_classes,\n",
    "            height=8,  # Adjusted based on encoder output size\n",
    "            width=8\n",
    "        )\n",
    "\n",
    "        # Final classifier (after ConvLSTM)\n",
    "        self.classifier = nn.Linear(64 * 8 * 8, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _, height, width = x.size()\n",
    "\n",
    "        # Process each frame through the encoder\n",
    "        x = x.view(batch_size * seq_len, 1, height, width)  # Combine batch and sequence dimensions\n",
    "        x = self.encoder(x)  # Shape: (batch_size * seq_len, 256, 8, 8)\n",
    "        _, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, seq_len, channels, height, width)  # Reshape for ConvLSTM\n",
    "\n",
    "        # Pass through ConvLSTM\n",
    "        x = self.convlstm(x)  # Last hidden state shape: (batch_size, hidden_channels[-1], height, width)\n",
    "\n",
    "        # Flatten the last hidden state for classification\n",
    "        x = x.view(batch_size, -1)  # Shape: (batch_size, hidden_channels[-1] * height * width)\n",
    "        x = self.classifier(x)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88cc30f1-afb5-44e1-b991-7c4671bc002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ComplexActionRecognitionModelConvLSTM(num_classes=10)\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef9cfcb5-b8ea-4b6c-bccd-f7377c7b67ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplexActionRecognitionModelConvLSTM(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout(p=0.3, inplace=False)\n",
       "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU()\n",
       "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (convlstm): ConvLSTM(\n",
       "    (layers): ModuleList(\n",
       "      (0): ConvLSTMCell(\n",
       "        (conv): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): ConvLSTMCell(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "719afda7-2320-4c29-aaba-872a2adbf78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:io1ejlav) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">own_convlstm2</strong> at: <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/io1ejlav' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/io1ejlav</a><br/> View project at: <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241130_180441-io1ejlav/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:io1ejlav). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/javeda1/MA-INF-4308-Lab-Vision-Systems/Assignment-4/wandb/run-20241130_180457-ltunhi4h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/ltunhi4h' target=\"_blank\">own_convlstm2</a></strong> to <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/ltunhi4h' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/ltunhi4h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█                                                       | 1/50 [00:14<11:58, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 1.4242, Train Accuracy: 0.3800\n",
      "Eval Loss: 2.0564, Eval Accuracy: 0.2667\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▏                                                     | 2/50 [00:29<11:38, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "Train Loss: 0.9252, Train Accuracy: 0.5866\n",
      "Eval Loss: 0.8451, Eval Accuracy: 0.6083\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███▎                                                    | 3/50 [00:43<11:25, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "Train Loss: 0.7879, Train Accuracy: 0.6347\n",
      "Eval Loss: 0.8245, Eval Accuracy: 0.6833\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████▍                                                   | 4/50 [00:58<11:11, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "Train Loss: 0.7394, Train Accuracy: 0.6806\n",
      "Eval Loss: 0.6501, Eval Accuracy: 0.7333\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▌                                                  | 5/50 [01:13<10:58, 14.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "Train Loss: 0.5893, Train Accuracy: 0.7474\n",
      "Eval Loss: 0.7111, Eval Accuracy: 0.6500\n",
      "Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████▋                                                 | 6/50 [01:27<10:42, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "Train Loss: 0.5226, Train Accuracy: 0.7745\n",
      "Eval Loss: 0.6526, Eval Accuracy: 0.7500\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████▊                                                | 7/50 [01:42<10:27, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "Train Loss: 0.4175, Train Accuracy: 0.8205\n",
      "Eval Loss: 0.6238, Eval Accuracy: 0.7583\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████▉                                               | 8/50 [01:56<10:13, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "Train Loss: 0.3705, Train Accuracy: 0.8434\n",
      "Eval Loss: 0.6709, Eval Accuracy: 0.7833\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████                                              | 9/50 [02:11<09:59, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "Train Loss: 0.3033, Train Accuracy: 0.8747\n",
      "Eval Loss: 0.4952, Eval Accuracy: 0.8000\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████                                            | 10/50 [02:26<09:45, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "Train Loss: 0.2668, Train Accuracy: 0.9040\n",
      "Eval Loss: 0.5566, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████                                           | 11/50 [02:40<09:30, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "Train Loss: 0.2057, Train Accuracy: 0.9353\n",
      "Eval Loss: 0.5320, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████▏                                         | 12/50 [02:55<09:16, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "Train Loss: 0.1573, Train Accuracy: 0.9478\n",
      "Eval Loss: 0.4889, Eval Accuracy: 0.8583\n",
      "Learning Rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████▎                                        | 13/50 [03:10<09:01, 14.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "Train Loss: 0.1094, Train Accuracy: 0.9749\n",
      "Eval Loss: 0.5505, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████▍                                       | 14/50 [03:24<08:47, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "Train Loss: 0.0896, Train Accuracy: 0.9749\n",
      "Eval Loss: 0.4895, Eval Accuracy: 0.8500\n",
      "Learning Rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████▌                                      | 15/50 [03:39<08:32, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "Train Loss: 0.0767, Train Accuracy: 0.9791\n",
      "Eval Loss: 0.4983, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████▌                                     | 16/50 [03:53<08:17, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "Train Loss: 0.0541, Train Accuracy: 0.9916\n",
      "Eval Loss: 0.4656, Eval Accuracy: 0.8500\n",
      "Learning Rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████▋                                    | 17/50 [04:08<08:03, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "Train Loss: 0.0432, Train Accuracy: 0.9937\n",
      "Eval Loss: 0.4701, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████████▊                                   | 18/50 [04:23<07:48, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "Train Loss: 0.0363, Train Accuracy: 0.9979\n",
      "Eval Loss: 0.4569, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████▉                                  | 19/50 [04:38<07:36, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "Train Loss: 0.0321, Train Accuracy: 0.9979\n",
      "Eval Loss: 0.4829, Eval Accuracy: 0.8250\n",
      "Learning Rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████                                 | 20/50 [04:52<07:22, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "Train Loss: 0.0268, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5532, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████████████                                | 21/50 [05:07<07:07, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "Train Loss: 0.0244, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5288, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████▏                              | 22/50 [05:22<06:51, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "Train Loss: 0.0229, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.4834, Eval Accuracy: 0.8667\n",
      "Learning Rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████▎                             | 23/50 [05:36<06:36, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "Train Loss: 0.0208, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5322, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████▍                            | 24/50 [05:51<06:21, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "Train Loss: 0.0201, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.4878, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████▌                           | 25/50 [06:06<06:07, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "Train Loss: 0.0187, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5003, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████████████▌                          | 26/50 [06:21<05:52, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "Train Loss: 0.0166, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5016, Eval Accuracy: 0.8167\n",
      "Learning Rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████████████████████████████▋                         | 27/50 [06:35<05:37, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "Train Loss: 0.0167, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5312, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████▊                        | 28/50 [06:50<05:24, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "Train Loss: 0.0166, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5142, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████▉                       | 29/50 [07:05<05:09, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "Train Loss: 0.0149, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5159, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████                      | 30/50 [07:19<04:54, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "Train Loss: 0.0159, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.4912, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████████████████                     | 31/50 [07:34<04:39, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "Train Loss: 0.0152, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.4872, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████▏                   | 32/50 [07:49<04:24, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "Train Loss: 0.0145, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.4811, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████▎                  | 33/50 [08:04<04:09, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "Train Loss: 0.0142, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.4924, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████▍                 | 34/50 [08:18<03:55, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "Train Loss: 0.0147, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5122, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████▌                | 35/50 [08:33<03:40, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "Train Loss: 0.0152, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5238, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████▌               | 36/50 [08:48<03:25, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n",
      "Train Loss: 0.0129, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5322, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████▋              | 37/50 [09:02<03:11, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "Train Loss: 0.0148, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5163, Eval Accuracy: 0.8500\n",
      "Learning Rate: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████████████████▊             | 38/50 [09:17<02:56, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "Train Loss: 0.0131, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5000, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████▉            | 39/50 [09:32<02:41, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "Train Loss: 0.0135, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5107, Eval Accuracy: 0.8500\n",
      "Learning Rate: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████           | 40/50 [09:47<02:27, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "Train Loss: 0.0123, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5541, Eval Accuracy: 0.8250\n",
      "Learning Rate: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████████████████          | 41/50 [10:01<02:12, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "Train Loss: 0.0134, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5320, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████▏        | 42/50 [10:16<01:57, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      "Train Loss: 0.0136, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5276, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████▎       | 43/50 [10:31<01:43, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "Train Loss: 0.0125, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5009, Eval Accuracy: 0.8250\n",
      "Learning Rate: 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████▍      | 44/50 [10:45<01:28, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n",
      "Train Loss: 0.0122, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5310, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████▌     | 45/50 [11:00<01:13, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "Train Loss: 0.0118, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5149, Eval Accuracy: 0.8500\n",
      "Learning Rate: 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████▌    | 46/50 [11:15<00:58, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "Train Loss: 0.0123, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5234, Eval Accuracy: 0.8667\n",
      "Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████▋   | 47/50 [11:30<00:44, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "Train Loss: 0.0125, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5138, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████▊  | 48/50 [11:44<00:29, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "Train Loss: 0.0126, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5360, Eval Accuracy: 0.8333\n",
      "Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████▉ | 49/50 [11:59<00:14, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "Train Loss: 0.0125, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.5110, Eval Accuracy: 0.8250\n",
      "Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 50/50 [12:14<00:00, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "Train Loss: 0.0116, Train Accuracy: 1.0000\n",
      "Eval Loss: 0.4885, Eval Accuracy: 0.8417\n",
      "Learning Rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>eval_accuracy</td><td>▁▅▆▆▅▇▇▇████████████▇███████████████████</td></tr><tr><td>eval_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>█████▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▆▆▇▇▇██████████████████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>eval_accuracy</td><td>0.84167</td></tr><tr><td>eval_loss</td><td>0.48846</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_accuracy</td><td>1</td></tr><tr><td>train_loss</td><td>0.01159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">own_convlstm2</strong> at: <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/ltunhi4h' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/ltunhi4h</a><br/> View project at: <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241130_180457-ltunhi4h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run training\n",
    "model, train_losses, train_accuracies, eval_losses, eval_accuracies, learning_rates = run_training(\n",
    "    model, train_loader, val_loader, criterion, device,\n",
    "    num_epochs=50, learning_rate=learning_rate, step_size=step_size, gamma=gamma, name='own_convlstm2', project='assignment_4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2596123-5c88-4578-8899-1da5a75a88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "height=64\n",
    "width=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ae21a-bf34-40a0-b969-5965cc28051d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d06333-fa5d-4cd9-b939-d785b75e7634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8e8f8-2396-4037-b4fd-d251ca4835db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f74c6db9-f7ed-434b-a7b3-2faaba10e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cfeecc-f82e-4364-852f-ef4e1c298503",
   "metadata": {},
   "source": [
    "https://github.com/kenshohara/video-classification-3d-cnn-pytorch/blob/master/models/pre_act_resnet.py#L10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a201aeda-c1d7-4df4-a243-0a71f6be529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34VideoClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=6, sample_size=64, sample_duration=16):\n",
    "        super(ResNet34VideoClassifier, self).__init__()\n",
    "\n",
    "        self.inplanes = 64  # Initialize inplanes\n",
    "\n",
    "        # Adjust the first convolution layer to handle 45 channels (for video frames)\n",
    "        self.conv1 = nn.Conv3d(45, 64, kernel_size=7, stride=(1, 2, 2),\n",
    "                               padding=(3, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
    "\n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(64, 3)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
    "\n",
    "        # Final adaptive average pooling\n",
    "        last_duration = math.ceil(sample_duration / 16)\n",
    "        last_size = math.ceil(sample_size / 32)\n",
    "        self.avgpool = nn.AvgPool3d((last_duration, last_size, last_size), stride=1)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(planes)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(self._basic_block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(self._basic_block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _basic_block(self, inplanes, planes, stride=1, downsample=None):\n",
    "        block = nn.Sequential(\n",
    "            nn.BatchNorm3d(inplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "        if downsample is not None:\n",
    "            block.add_module(\"downsample\", downsample)\n",
    "        \n",
    "        return block\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # Input shape should be [batch_size, 45, 64, 64, 64]\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84dfd00e-2786-4630-a9e6-397875f1b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ResNet34 model\n",
    "model = ResNet34VideoClassifier(num_classes=num_classes, sample_size=64, sample_duration=45).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3b447c0-aebe-4eb8-9a6a-ab3b169ce8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet34VideoClassifier(\n",
       "  (conv1): Conv3d(45, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
       "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool3d(kernel_size=(3, 2, 2), stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbdcf366-157f-4d3c-b8bc-4877818a5f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dpry38gh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resent3d</strong> at: <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/dpry38gh' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/dpry38gh</a><br/> View project at: <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241130_173002-dpry38gh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dpry38gh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/javeda1/MA-INF-4308-Lab-Vision-Systems/Assignment-4/wandb/run-20241130_173103-iaq4jom4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/iaq4jom4' target=\"_blank\">resent3d</a></strong> to <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmadjaved97/assignment_4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmadjaved97/assignment_4/runs/iaq4jom4' target=\"_blank\">https://wandb.ai/ahmadjaved97/assignment_4/runs/iaq4jom4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                | 0/10 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 64, 1, 1, 1], expected input[16, 128, 1, 8, 8] to have 64 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model, train_losses, train_accuracies, eval_losses, eval_accuracies, learning_rates \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresent3d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43massignment_4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 81\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(model, train_loader, eval_loader, criterion, device, num_epochs, learning_rate, step_size, gamma, project, name)\u001b[0m\n\u001b[1;32m     77\u001b[0m learning_rates \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Evaluate after each epoch\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     eval_loss, eval_accuracy \u001b[38;5;241m=\u001b[39m evaluate_nn(model, eval_loader, criterion, device)\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtrain_nn\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get outputs for the full sequence\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Only use the last time step's output for classification\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Use the last timestep output\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 76\u001b[0m, in \u001b[0;36mResNet34VideoClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m     75\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m---> 76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m     78\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:603\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    593\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    594\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    602\u001b[0m     )\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 64, 1, 1, 1], expected input[16, 128, 1, 8, 8] to have 64 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model, train_losses, train_accuracies, eval_losses, eval_accuracies, learning_rates = run_training(\n",
    "    model, train_loader, val_loader, criterion, device,\n",
    "    num_epochs=num_epochs, learning_rate=learning_rate, step_size=step_size, gamma=gamma, name='resent3d', project='assignment_4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75009fb5-e87f-4206-bc04-572dc3587850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
