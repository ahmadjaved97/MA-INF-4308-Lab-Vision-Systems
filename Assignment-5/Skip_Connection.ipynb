{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c84e63-3fdb-40cd-bcd5-8e7a64989b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f78306-cbc7-46fb-86e2-d719c359a89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Used: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device Used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90400f72-d7ca-41e6-9382-6f98bd452b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFHQDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: path of the parent directory that contains images.\n",
    "        transforms: augmentations applied to the images (can be none or more).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_mapping = {}\n",
    "        \n",
    "        extensions = (\".jpg\", \".jpeg\", \".png\")\n",
    "        # go through all sub-directories\n",
    "        for label, category in enumerate(sorted(os.listdir(root_dir))):\n",
    "            full_path = os.path.join(root_dir, category)\n",
    "            if os.path.exists(full_path):\n",
    "                self.class_mapping[label] = category\n",
    "                for img_name in os.listdir(full_path):\n",
    "                    if img_name.endswith(extensions):\n",
    "                        self.image_paths.append(os.path.join(full_path, img_name))\n",
    "                        self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b666bbd0-0228-4b06-8b1a-5bc37bc7d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Hyperparameters\n",
    "img_size = 64\n",
    "batch_size = 64\n",
    "\n",
    "# dataset paths\n",
    "train_dir = '/home/user/javeda1/stargan-v2/data/afhq/train'\n",
    "val_dir = '/home/user/javeda1/stargan-v2/data/afhq/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "751e1940-1e27-4f56-91fd-e0ec0914c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    \n",
    "        # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        transforms.Resize((img_size, img_size)), # image is downsampled to 64x64\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9157a34f-268b-49e6-8669-a76b76606245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14630\n",
      "Validation dataset size: 1500\n"
     ]
    }
   ],
   "source": [
    "# Load the train and val dataset\n",
    "train_dataset = AFHQDataset(root_dir=train_dir, transform=transform)\n",
    "val_dataset = AFHQDataset(root_dir=val_dir, transform=transform)\n",
    "\n",
    "# DataLoaders for train and val sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "train_size = len(train_loader.dataset)\n",
    "val_size = len(val_loader.dataset)\n",
    "\n",
    "print(f\"Train dataset size: {train_size}\")\n",
    "print(f\"Validation dataset size: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c448c9d-ac9b-4781-bbfd-7a57bcb8f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(recon_x, x, mu, log_var, kl_weight=1):\n",
    "    \"\"\"\n",
    "    Calculates the VAE loss as a combination of \n",
    "    reconstruction loss and KL divergence, \n",
    "    scaled by a weight.\n",
    "    \"\"\"\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + kl_divergence * kl_weight, recon_loss, kl_divergence\n",
    "\n",
    "def train_vae(model, train_loader, optimizer, kl_weight, device):\n",
    "    \"\"\"\n",
    "    Trains the Variational Autoencoder (VAE) for one epoch on the given training data loader.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_recon_loss = 0.0\n",
    "    running_kl_loss = 0.0\n",
    "    \n",
    "    for inputs, _ in train_loader:  #labels not used\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, mu, log_var = model(inputs)\n",
    "        loss, recon_loss, kl_loss = vae_loss_function(recon_x, inputs, mu, log_var, kl_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Collect all losses\n",
    "        running_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "        running_kl_loss += kl_loss.item()\n",
    "        \n",
    "    dataset_size = len(train_loader.dataset)\n",
    "    return {\n",
    "        'total_loss': running_loss / dataset_size,\n",
    "        'recon_loss': running_recon_loss / dataset_size,\n",
    "        'kl_loss': running_kl_loss / dataset_size\n",
    "    }\n",
    "\n",
    "def evaluate_vae(model, val_loader, kl_weight, device):\n",
    "    \"\"\"\n",
    "    Evaluates the Variational Autoencoder (VAE) on the validation dataset after each epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_recon_loss = 0.0\n",
    "    running_kl_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon_x, mu, log_var = model(inputs)\n",
    "            loss, recon_loss, kl_loss = vae_loss_function(recon_x, inputs, mu, log_var, kl_weight)\n",
    "            \n",
    "            # Collect all losses\n",
    "            running_loss += loss.item()\n",
    "            running_recon_loss += recon_loss.item()\n",
    "            running_kl_loss += kl_loss.item()\n",
    "            \n",
    "    dataset_size = len(val_loader.dataset)\n",
    "    return {\n",
    "        'total_loss': running_loss / dataset_size,\n",
    "        'recon_loss': running_recon_loss / dataset_size,\n",
    "        'kl_loss': running_kl_loss / dataset_size\n",
    "    }\n",
    "\n",
    "def run_vae_training(\n",
    "    model, train_loader, val_loader, device, num_epochs, learning_rate=0.001, \n",
    "    project=\"vae-training\", name=\"vae_run\", kl_weight=0.1, step_size=30, gamma=0.1):\n",
    "\n",
    "    \"\"\"Train and evaluate the model for a given number of epochs with W&B logging\"\"\"\n",
    "\n",
    "    print(f\"Training Name: {name}\")\n",
    "    print(f\"Total num. of Epochs: {num_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"KL Weight used for Loss function: {kl_weight}\\n\")\n",
    "    \n",
    "    # Sample a batch for visualization ( used here to make it same for every epoch)\n",
    "    inputs, _ = next(iter(train_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    # Initialize W&B logging\n",
    "    wandb.init(project=project, name=name, \n",
    "               config={\n",
    "                   \"learning_rate\": learning_rate,\n",
    "                   \"num_epochs\": num_epochs,\n",
    "                   \"step_size\": step_size,\n",
    "                   \"gamma\": gamma,\n",
    "                   \"kl_weight\": kl_weight\n",
    "               })\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5) # Optimizer\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)  # Learning rate scheduler\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Train for one epoch\n",
    "        train_metrics = train_vae(model, train_loader, optimizer, kl_weight, device)\n",
    "        # Evaluate after each epoch\n",
    "        val_metrics = evaluate_vae(model, val_loader, kl_weight, device)\n",
    "        \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log images to W&B\n",
    "        with torch.no_grad():\n",
    "            # # Sample a small batch for visualization\n",
    "            # inputs, _ = next(iter(train_loader))\n",
    "            # inputs = inputs[:32].to(device)\n",
    "            recon_x, _, _ = model(inputs)\n",
    "\n",
    "            # Normalize and convert to image format\n",
    "            recon_x = recon_x.view(-1, *inputs.shape[1:])\n",
    "            recon_grid = make_grid(recon_x.cpu().detach() * 0.5 + 0.5, normalize=True, pad_value=1, padding=10)\n",
    "            original_grid = make_grid(inputs.cpu().detach() * 0.5 + 0.5, normalize=True, pad_value=1, padding=10)\n",
    "    \n",
    "        # Log all data to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train/total_loss\": train_metrics['total_loss'],\n",
    "            \"train/recon_loss\": train_metrics['recon_loss'],\n",
    "            \"train/kl_loss\": train_metrics['kl_loss'],\n",
    "            \"val/total_loss\": val_metrics['total_loss'],\n",
    "            \"val/recon_loss\": val_metrics['recon_loss'],\n",
    "            \"val/kl_loss\": val_metrics['kl_loss'],\n",
    "            \"learning_rate\": current_lr,\n",
    "            \"original_images\": wandb.Image(original_grid),\n",
    "            \"reconstructed_images\": wandb.Image(recon_grid),\n",
    "        })\n",
    "        \n",
    "        # # Print stats after each epoch\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        print(f\"Train - Total: {train_metrics['total_loss']:.4f}, \"\n",
    "              f\"Recon: {train_metrics['recon_loss']:.4f}, \"\n",
    "              f\"KL: {train_metrics['kl_loss']:.4f}\")\n",
    "        print(f\"Eval  - Total: {val_metrics['total_loss']:.4f}, \"\n",
    "              f\"Recon: {val_metrics['recon_loss']:.4f}, \"\n",
    "              f\"KL: {val_metrics['kl_loss']:.4f}\")\n",
    "    \n",
    "    # End W&B run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a520c2-cf19-4b4d-98b6-bb0fd0e6f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv_initial = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
    "        \n",
    "        # Downsampling blocks\n",
    "        self.conv1 = nn.Conv2d(64, 128, 4, stride=2, padding=1)   \n",
    "        self.conv2 = nn.Conv2d(128, 256, 4, stride=2, padding=1)  \n",
    "        self.conv3 = nn.Conv2d(256, 512, 4, stride=2, padding=1)  \n",
    "        self.conv4 = nn.Conv2d(512, 1024, 4, stride=2, padding=1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        self.bn4 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.fc_mu = nn.Linear(1024 * 4 * 4, latent_dim)\n",
    "        self.fc_var = nn.Linear(1024 * 4 * 4, latent_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial convolution\n",
    "        x = F.leaky_relu(self.conv_initial(x), 0.2)\n",
    "        \n",
    "        # Downsampling path\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
    "        \n",
    "        # Save the feature map for the skip connection\n",
    "        skip_connection = F.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n",
    "        \n",
    "        x = F.leaky_relu(self.bn4(self.conv4(skip_connection)), 0.2)\n",
    "        \n",
    "        # Flatten and apply dropout\n",
    "        x = self.dropout(x.view(x.size(0), -1))\n",
    "        \n",
    "        # Generate latent parameters\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        \n",
    "        return mu, log_var, skip_connection\n",
    "\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        \n",
    "        # Initial fully connected layer\n",
    "        self.fc = nn.Linear(latent_dim, 1024 * 4 * 4)\n",
    "        \n",
    "        # Upsampling blocks\n",
    "        self.conv1 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1) \n",
    "        self.conv2 = nn.ConvTranspose2d(1024, 256, 4, stride=2, padding=1)  \n",
    "        self.conv3 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)  \n",
    "        self.conv4 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Final convolution for output\n",
    "        self.conv_final = nn.Conv2d(64, 3, 3, stride=1, padding=1)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, z, skip_connection):\n",
    "        # Reshape from latent space\n",
    "        x = F.relu(self.fc(z))\n",
    "        x = x.view(x.size(0), 1024, 4, 4)\n",
    "        \n",
    "        # Upsampling path\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # # Apply skip connection\n",
    "        x = torch.cat([x, skip_connection], dim=1)  # Concatenate along channel dimension\n",
    "        # Apply skip connection if provided\n",
    "        # if skip_connection is not None:\n",
    "        #     x = torch.cat([x, skip_connection], dim=1)  # Concatenate along channel dimension\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Final convolution with tanh activation\n",
    "        x = torch.tanh(self.conv_final(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.encoder = VAEEncoder(latent_dim)\n",
    "        self.decoder = VAEDecoder(latent_dim)\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * log_var)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        return mu\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder with skip connection\n",
    "        mu, log_var, skip_connection = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # Decoder with skip connection\n",
    "        recon_x = self.decoder(z, skip_connection)\n",
    "        return recon_x, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a02422-c227-4687-ac5a-dcb972d50788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d865d5-f752-4240-9df2-43aea13c5e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Name: run_kl_wgt_0.01_ep_40_latent_dim_512_skip_cn\n",
      "Total num. of Epochs: 40\n",
      "Learning Rate: 0.0001\n",
      "KL Weight used for Loss function: 0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                      | 1/40 [00:11<07:38, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/40\n",
      "Learning Rate: 0.000100\n",
      "Train - Total: 365.4440, Recon: 363.4789, KL: 196.5119\n",
      "Eval  - Total: 126.2105, Recon: 125.7657, KL: 44.4807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▊                                                     | 2/40 [00:26<08:41, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/40\n",
      "Learning Rate: 0.000100\n",
      "Train - Total: 112.9381, Recon: 112.4148, KL: 52.3299\n",
      "Eval  - Total: 79.9486, Recon: 79.6469, KL: 30.1692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████▏                                                   | 3/40 [00:36<07:20, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/40\n",
      "Learning Rate: 0.000100\n",
      "Train - Total: 83.8239, Recon: 83.4087, KL: 41.5205\n",
      "Eval  - Total: 83.0544, Recon: 82.7761, KL: 27.8362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▌                                                  | 4/40 [00:51<07:51, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/40\n",
      "Learning Rate: 0.000100\n",
      "Train - Total: 68.7571, Recon: 68.4081, KL: 34.8940\n",
      "Eval  - Total: 69.1650, Recon: 68.9845, KL: 18.0593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████                                                 | 5/40 [01:01<06:55, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/40\n",
      "Learning Rate: 0.000100\n",
      "Train - Total: 60.6038, Recon: 60.2789, KL: 32.4947\n",
      "Eval  - Total: 55.8070, Recon: 55.6468, KL: 16.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████▍                                               | 6/40 [01:16<07:28, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/40\n",
      "Learning Rate: 0.000100\n",
      "Train - Total: 54.0693, Recon: 53.7859, KL: 28.3386\n",
      "Eval  - Total: 50.0897, Recon: 49.9214, KL: 16.8365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████▊                                              | 7/40 [01:29<07:09, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/40\n",
      "Learning Rate: 0.000100\n",
      "Train - Total: 49.2945, Recon: 49.0250, KL: 26.9478\n",
      "Eval  - Total: 52.4603, Recon: 52.3126, KL: 14.7680\n"
     ]
    }
   ],
   "source": [
    "# Model training parameters\n",
    "learning_rate=0.0001\n",
    "step_size=10\n",
    "gamma=0.5\n",
    "\n",
    "kl_weight=0.01\n",
    "\n",
    "num_epochs=40\n",
    "\n",
    "latent_dim = 512 # define latent dimension\n",
    "\n",
    "name=f\"run_kl_wgt_{str(kl_weight)}_ep_{num_epochs}_latent_dim_{latent_dim}_skip_cn\"\n",
    "project=\"assignment-5-v2\"\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = ConvVAE(latent_dim=latent_dim).to(device)\n",
    "# model\n",
    "\n",
    "model = run_vae_training(\n",
    "    model, train_loader, val_loader, device, \n",
    "    num_epochs=num_epochs, learning_rate=learning_rate,\n",
    "    step_size=step_size, gamma=gamma,\n",
    "    kl_weight=kl_weight,\n",
    "    name=name, project=project\n",
    ")\n",
    "\n",
    "save_path = os.path.join(saved_model_folder, name)\n",
    "torch.save(model, save_path)\n",
    "print(f\"Model saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d1069f-7701-4cae-8103-eb2c958b5b82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(save_path)\n\u001b[1;32m      2\u001b[0m max_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(val_loader\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[1;32m      3\u001b[0m fid_score \u001b[38;5;241m=\u001b[39m compute_fid_score(model, val_loader, device, max_samples\u001b[38;5;241m=\u001b[39mmax_samples)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model(save_path)\n",
    "max_samples = min(2000, len(val_loader.dataset))\n",
    "fid_score = compute_fid_score(model, val_loader, device, max_samples=max_samples)\n",
    "print(f\"FID Score for model {name}: {fid_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "207bd022-6e97-4561-bffb-065ebdc5fc75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[43mval_loader\u001b[49m)\n\u001b[1;32m      2\u001b[0m images, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[1;32m      3\u001b[0m visualize_reconstructions(model, images, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "data_iter = iter(val_loader)\n",
    "images, _ = next(data_iter)\n",
    "visualize_reconstructions(model, images, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e50c32-9bae-47b4-8cd9-4ef795403995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
